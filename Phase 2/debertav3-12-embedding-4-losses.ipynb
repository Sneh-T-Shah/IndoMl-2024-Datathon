{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523},{"sourceId":9733036,"sourceType":"datasetVersion","datasetId":5956483}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":21448.848322,"end_time":"2024-10-26T18:29:10.763363","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-26T12:31:41.915041","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"0e4f2339adbf4e65822ed0fac0477e77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18bfc21f6e884d1982f2adb14d38f8c9","placeholder":"​","style":"IPY_MODEL_c16a0f9381d442fd84a5b2a473786466","value":" 2.46M/2.46M [00:00&lt;00:00, 40.8MB/s]"}},"0f74a9b630fc45239de19670e4e838e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18bfc21f6e884d1982f2adb14d38f8c9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c46fa0391fc46ae81bb9a44759fb010":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26baf18ea4f64eed864b4af831fd439b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"396776f9893a49b99bdfbddc25afe03b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ec088da6df45a880f4bd699d699c91","placeholder":"​","style":"IPY_MODEL_3c0e46d9bf5b4525b21ac21222b91e68","value":"spm.model: 100%"}},"3c0e46d9bf5b4525b21ac21222b91e68":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cfcbff5051e48288414ed68078d039f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_deb0edea48ea4d53accd228bb9fa3f20","IPY_MODEL_8f185ae6499a4b6a9c1957b67c66497d","IPY_MODEL_43793703a2a64b98be6115eabf88e894"],"layout":"IPY_MODEL_c56df4725cd3417687c3fb1f6f222096"}},"433295f1d05a46eb91d877d1ca3ec048":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43793703a2a64b98be6115eabf88e894":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e049a475830745148e927f5c7c261163","placeholder":"​","style":"IPY_MODEL_b2782b81deb9430a8f6225581678fb1b","value":" 371M/371M [00:01&lt;00:00, 228MB/s]"}},"45318db96a0a49e68f0d4a45ce1f9dc4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c6d758d0ee84905aea3706b99eddbfb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d33121d893ce4440a2f289ded5eed8d6","IPY_MODEL_830aaff35a744f1ea91490361f7f67c4","IPY_MODEL_7d5d7b12c4054237abdff75a566bab49"],"layout":"IPY_MODEL_e57b7fad23734038809884c1305ceb68"}},"52528fd4744e4377a49da1917d7f4220":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a14dc9ca65e41679c5ad030f564a781":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61a624d0ae044a52b9e5d72f694f7f1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70a3044f9c0542bc934bbf06ed4eed93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73a88dfe1fda4cf7a682457cda768365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f74a9b630fc45239de19670e4e838e5","placeholder":"​","style":"IPY_MODEL_1c46fa0391fc46ae81bb9a44759fb010","value":" 579/579 [00:00&lt;00:00, 34.9kB/s]"}},"7d5d7b12c4054237abdff75a566bab49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a14dc9ca65e41679c5ad030f564a781","placeholder":"​","style":"IPY_MODEL_9e0aedfeb4054fffa34e33f658460b77","value":" 52.0/52.0 [00:00&lt;00:00, 4.36kB/s]"}},"805586d5b216432a81955376ccaf3c2f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8198588f15704e0b9d900e8556ec6799":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830aaff35a744f1ea91490361f7f67c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8198588f15704e0b9d900e8556ec6799","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b67c5a4837a64acdb018b761a3e90674","value":52}},"8f185ae6499a4b6a9c1957b67c66497d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61a624d0ae044a52b9e5d72f694f7f1e","max":371146213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_805586d5b216432a81955376ccaf3c2f","value":371146213}},"9555e23d945d4c1884d79ee538d27cb3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a08a71c249eb4c838c5982d21d0e90ea","placeholder":"​","style":"IPY_MODEL_26baf18ea4f64eed864b4af831fd439b","value":"config.json: 100%"}},"9e0aedfeb4054fffa34e33f658460b77":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a08a71c249eb4c838c5982d21d0e90ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad09b1fec69b43d1a140377d9908a2f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2782b81deb9430a8f6225581678fb1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ec088da6df45a880f4bd699d699c91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b67c5a4837a64acdb018b761a3e90674":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bad25572e4c8480c83bbae461ba08119":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0e4d1e2bc9e4ee280a77aab1dc3c35b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16a0f9381d442fd84a5b2a473786466":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c56df4725cd3417687c3fb1f6f222096":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6cffad2bd9649a0a5437c4a873bc310":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_396776f9893a49b99bdfbddc25afe03b","IPY_MODEL_d57a123cc2514222884f0c834637e4f9","IPY_MODEL_0e4f2339adbf4e65822ed0fac0477e77"],"layout":"IPY_MODEL_ea42d2c99e5f457dbf10033a4a56aa0c"}},"cfffa06a5e71472891b1bf5979d0b069":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdd5008747c947ffa49d1d70c33b26a4","max":579,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7b700edad994da8957762e27550dae7","value":579}},"d33121d893ce4440a2f289ded5eed8d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bad25572e4c8480c83bbae461ba08119","placeholder":"​","style":"IPY_MODEL_52528fd4744e4377a49da1917d7f4220","value":"tokenizer_config.json: 100%"}},"d57a123cc2514222884f0c834637e4f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_433295f1d05a46eb91d877d1ca3ec048","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45318db96a0a49e68f0d4a45ce1f9dc4","value":2464616}},"deb0edea48ea4d53accd228bb9fa3f20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70a3044f9c0542bc934bbf06ed4eed93","placeholder":"​","style":"IPY_MODEL_ad09b1fec69b43d1a140377d9908a2f8","value":"pytorch_model.bin: 100%"}},"e049a475830745148e927f5c7c261163":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e57b7fad23734038809884c1305ceb68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b700edad994da8957762e27550dae7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea42d2c99e5f457dbf10033a4a56aa0c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6d4cbc1e4fb432d97b3216a8d30d7f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9555e23d945d4c1884d79ee538d27cb3","IPY_MODEL_cfffa06a5e71472891b1bf5979d0b069","IPY_MODEL_73a88dfe1fda4cf7a682457cda768365"],"layout":"IPY_MODEL_c0e4d1e2bc9e4ee280a77aab1dc3c35b"}},"fdd5008747c947ffa49d1d70c33b26a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.406099,"end_time":"2024-10-26T12:31:56.085114","exception":false,"start_time":"2024-10-26T12:31:44.679015","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:38.170162Z","iopub.execute_input":"2024-10-27T08:08:38.170712Z","iopub.status.idle":"2024-10-27T08:08:48.056139Z","shell.execute_reply.started":"2024-10-27T08:08:38.170653Z","shell.execute_reply":"2024-10-27T08:08:48.055128Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"papermill":{"duration":0.168313,"end_time":"2024-10-26T12:31:56.264435","exception":false,"start_time":"2024-10-26T12:31:56.096122","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:48.057925Z","iopub.execute_input":"2024-10-27T08:08:48.058262Z","iopub.status.idle":"2024-10-27T08:08:48.211514Z","shell.execute_reply.started":"2024-10-27T08:08:48.058201Z","shell.execute_reply":"2024-10-27T08:08:48.210194Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# df = df[:100]","metadata":{"papermill":{"duration":0.017816,"end_time":"2024-10-26T12:31:56.293025","exception":false,"start_time":"2024-10-26T12:31:56.275209","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:48.212808Z","iopub.execute_input":"2024-10-27T08:08:48.213126Z","iopub.status.idle":"2024-10-27T08:08:48.217728Z","shell.execute_reply.started":"2024-10-27T08:08:48.213093Z","shell.execute_reply":"2024-10-27T08:08:48.216797Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])","metadata":{"papermill":{"duration":0.815436,"end_time":"2024-10-26T12:31:57.119032","exception":false,"start_time":"2024-10-26T12:31:56.303596","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:48.219954Z","iopub.execute_input":"2024-10-27T08:08:48.220291Z","iopub.status.idle":"2024-10-27T08:08:49.005692Z","shell.execute_reply.started":"2024-10-27T08:08:48.220242Z","shell.execute_reply":"2024-10-27T08:08:49.004704Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from transformers import DebertaModel, DebertaTokenizer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom torch.distributions import Categorical\nimport numpy as np\nimport time\nfrom torch.profiler import profile, record_function, ProfilerActivity","metadata":{"papermill":{"duration":1.463794,"end_time":"2024-10-26T12:31:58.593574","exception":false,"start_time":"2024-10-26T12:31:57.129780","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T10:06:02.369562Z","iopub.execute_input":"2024-10-27T10:06:02.369942Z","iopub.status.idle":"2024-10-27T10:06:02.376725Z","shell.execute_reply.started":"2024-10-27T10:06:02.369904Z","shell.execute_reply":"2024-10-27T10:06:02.375824Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# For using the new one\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\n# For loading the saved one\n# model_dir = \"/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model\"\n# tokenizer = DebertaTokenizer.from_pretrained(model_dir)","metadata":{"papermill":{"duration":2.258286,"end_time":"2024-10-26T12:32:00.862793","exception":false,"start_time":"2024-10-26T12:31:58.604507","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:50.579193Z","iopub.execute_input":"2024-10-27T08:08:50.579656Z","iopub.status.idle":"2024-10-27T08:08:52.759867Z","shell.execute_reply.started":"2024-10-27T08:08:50.579622Z","shell.execute_reply":"2024-10-27T08:08:52.758991Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1e2041e1004326a50535a60fb39235"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c7148c37f24a6e9a28205c5edb3e1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb1d8db6dfa44357a97c372d7e38d967"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:558: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"MAX_LENGTH = 12\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 10\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"papermill":{"duration":0.080813,"end_time":"2024-10-26T12:32:00.955221","exception":false,"start_time":"2024-10-26T12:32:00.874408","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:52.761024Z","iopub.execute_input":"2024-10-27T08:08:52.761506Z","iopub.status.idle":"2024-10-27T08:08:52.796110Z","shell.execute_reply.started":"2024-10-27T08:08:52.761455Z","shell.execute_reply":"2024-10-27T08:08:52.795143Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"papermill":{"duration":0.019126,"end_time":"2024-10-26T12:32:00.985859","exception":false,"start_time":"2024-10-26T12:32:00.966733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:52.797351Z","iopub.execute_input":"2024-10-27T08:08:52.797649Z","iopub.status.idle":"2024-10-27T08:08:52.807473Z","shell.execute_reply.started":"2024-10-27T08:08:52.797617Z","shell.execute_reply":"2024-10-27T08:08:52.806659Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"papermill":{"duration":0.025522,"end_time":"2024-10-26T12:32:01.022748","exception":false,"start_time":"2024-10-26T12:32:00.997226","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:52.808519Z","iopub.execute_input":"2024-10-27T08:08:52.808824Z","iopub.status.idle":"2024-10-27T08:08:52.820233Z","shell.execute_reply.started":"2024-10-27T08:08:52.808792Z","shell.execute_reply":"2024-10-27T08:08:52.819344Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\n\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\n\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"papermill":{"duration":23.419184,"end_time":"2024-10-26T12:32:24.454502","exception":false,"start_time":"2024-10-26T12:32:01.035318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:08:52.823115Z","iopub.execute_input":"2024-10-27T08:08:52.823441Z","iopub.status.idle":"2024-10-27T08:09:17.133877Z","shell.execute_reply.started":"2024-10-27T08:08:52.823409Z","shell.execute_reply":"2024-10-27T08:09:17.132983Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from transformers import DebertaModel, DebertaTokenizer\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.distributions import Categorical\nimport os\n\nclass AdvancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands):\n        super().__init__()\n        # Load the base model\n        self.deberta = AutoModel.from_pretrained(\n            \"microsoft/deberta-v3-base\",\n            num_labels=1,  # Set to None to get the base model without classification head\n            output_hidden_states=True  # Enable output of hidden states\n        )\n        \n        # Remove the classification head if it exists\n        if hasattr(self.deberta, 'classifier'):\n            delattr(self.deberta, 'classifier')\n        \n        hidden_size = self.deberta.config.hidden_size\n        \n        # Classification heads for different levels\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size, num_groups)\n        self.module_classifier = nn.Linear(hidden_size, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size, num_brands)\n\n        # Pooler layer\n        self.pooler = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.Tanh()\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Get the base model outputs\n        outputs = self.deberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        \n        # Get the last hidden state and apply pooling\n        last_hidden_state = outputs.hidden_states[-1]\n        cls_token = last_hidden_state[:, 0, :]  # Get [CLS] token\n        pooled_output = self.pooler(cls_token)\n        \n        # Generate predictions for each level\n        supergroup_logits = self.supergroup_classifier(pooled_output)\n        group_logits = self.group_classifier(pooled_output)\n        module_logits = self.module_classifier(pooled_output)\n        brand_logits = self.brand_classifier(pooled_output)\n        \n        return {\n            'supergroup': supergroup_logits,\n            'group': group_logits,\n            'module': module_logits,\n            'brand': brand_logits\n        }\n\n    def get_embeddings(self, input_ids, attention_mask):\n        outputs = self.deberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n        last_hidden_state = outputs.hidden_states[-1]\n        cls_token = last_hidden_state[:, 0, :]\n        return self.pooler(cls_token)","metadata":{"papermill":{"duration":0.026961,"end_time":"2024-10-26T12:32:24.492812","exception":false,"start_time":"2024-10-26T12:32:24.465851","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:09:17.135058Z","iopub.execute_input":"2024-10-27T08:09:17.135368Z","iopub.status.idle":"2024-10-27T08:09:17.148129Z","shell.execute_reply.started":"2024-10-27T08:09:17.135335Z","shell.execute_reply":"2024-10-27T08:09:17.147270Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class JointAccuracyTrainer:\n    def __init__(self, model, supervised_optimizer, device):\n        self.model = model\n        self.supervised_optimizer = supervised_optimizer\n        self.device = device\n        self.best_accuracy = 0\n        self.best_model_state = None\n\n    def supervised_step(self, batch, true_labels):\n        self.supervised_optimizer.zero_grad()\n        \n        # Forward pass through the model\n        outputs = self.model(\n            batch['input_ids'],\n            batch['attention_mask']\n        )\n        \n        # Compute loss for each level\n        total_loss = self.compute_joint_loss(outputs, true_labels)\n        \n        # Backward pass\n        total_loss.backward()\n        self.supervised_optimizer.step()\n        \n        return total_loss.item()\n\n    def compute_joint_loss(self, outputs, true_labels):\n        losses = []\n        weights = {\n            'supergroup': 1.0,\n            'group': 1.0,\n            'module': 1.0,\n            'brand': 1.0\n        }\n        \n        for level, logits in outputs.items():\n            if level in true_labels:\n                loss = F.cross_entropy(logits, true_labels[level])\n                losses.append(weights[level] * loss)\n        \n        return sum(losses)\n\n    def compute_accuracy(self, outputs, true_labels):\n        accuracies = {}\n        all_correct = True\n        for level, logits in outputs.items():\n            if level in true_labels:\n                predictions = torch.argmax(logits, dim=1)\n                correct = (predictions == true_labels[level]).float().mean()\n                accuracies[level] = correct.item()\n                all_correct &= (predictions == true_labels[level]).all().item()\n        accuracies['item'] = float(all_correct)\n        return accuracies\n\n    def save_best_model(self, current_accuracy):\n        if current_accuracy > self.best_accuracy:\n            self.best_accuracy = current_accuracy\n            self.best_model_state = {\n                key: value.cpu().clone() for key, value in self.model.state_dict().items()\n            }","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:06:12.492477Z","iopub.execute_input":"2024-10-27T10:06:12.493161Z","iopub.status.idle":"2024-10-27T10:06:12.505474Z","shell.execute_reply.started":"2024-10-27T10:06:12.493120Z","shell.execute_reply":"2024-10-27T10:06:12.504559Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Usage\nmodel = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\n# For loading the model saved after pre-training the saved dict\nmodel_path = '/kaggle/input/debertav3-12-embedding-tmp/New_model/model.pth'\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:06:12.631138Z","iopub.execute_input":"2024-10-27T10:06:12.631433Z","iopub.status.idle":"2024-10-27T10:06:13.667790Z","shell.execute_reply.started":"2024-10-27T10:06:12.631402Z","shell.execute_reply":"2024-10-27T10:06:13.666869Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1464995192.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Profile on a single sample before starting the full training\nmodel.train()\nsample_batch = next(iter(train_loader))  # Get one sample batch\nsample_batch = {k: v.to(device) for k, v in sample_batch.items()}\ntrue_labels = {\n    'supergroup': sample_batch['labels1'],\n    'group1': sample_batch['labels2'],\n    'group2': sample_batch['labels3'],\n    'group3': sample_batch['labels4']\n}\ndevice = next(model.parameters()).device\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ntrainer = JointAccuracyTrainer(model, optimizer, device)\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n    _ = trainer.supervised_step(sample_batch, true_labels)\n\nprint(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\nprint(\"GFLOPs during training\") #GigaFLOPs\nprint(sum(k.flops for k in prof.key_averages())/1e9)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:07:18.334139Z","iopub.execute_input":"2024-10-27T10:07:18.334555Z","iopub.status.idle":"2024-10-27T10:07:22.594993Z","shell.execute_reply.started":"2024-10-27T10:07:18.334519Z","shell.execute_reply":"2024-10-27T10:07:22.593941Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                               aten::mm         2.89%       5.985ms         2.89%       5.985ms      30.535us       0.000us         0.00%       0.000us       0.000us           196    290067.579  \n                                            aten::addmm         2.00%       4.131ms         2.60%       5.378ms      53.248us       0.000us         0.00%       0.000us       0.000us           101    145658.610  \n                                              aten::bmm         1.93%       3.988ms         2.14%       4.425ms      30.730us       0.000us         0.00%       0.000us       0.000us           144     44505.760  \n                                              aten::mul         1.23%       2.540ms         1.64%       3.388ms      22.145us       0.000us         0.00%       0.000us       0.000us           153        44.090  \n                                              aten::add         1.11%       2.304ms         1.38%       2.861ms      21.351us       0.000us         0.00%       0.000us       0.000us           134        35.706  \n                    Optimizer.zero_grad#AdamW.zero_grad         0.09%     189.018us         0.09%     189.018us     189.018us       0.000us         0.00%       0.000us       0.000us             1            --  \n                                            aten::zeros         0.08%     156.046us         1.08%       2.226ms     130.945us       0.000us         0.00%       0.000us       0.000us            17            --  \n                                            aten::empty         2.53%       5.247ms         2.72%       5.626ms       7.665us       0.000us         0.00%       0.000us       0.000us           734            --  \n                                            aten::zero_         0.71%       1.463ms         3.60%       7.462ms      16.693us       0.000us         0.00%       0.000us       0.000us           447            --  \n                                            aten::fill_         1.28%       2.653ms         2.90%       6.015ms      13.425us       0.000us         0.00%       0.000us       0.000us           448            --  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 207.061ms\nSelf CUDA time total: 246.543ms\n\nGFLOPs during training\n480.311745111\n","output_type":"stream"}]},{"cell_type":"code","source":"# TODO, make a check on item accuracy, if the test accuracy is good one\n\ndef train_and_evaluate(model, train_loader, val_loader, num_epochs):\n    device = next(model.parameters()).device\n    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n    trainer = JointAccuracyTrainer(model, optimizer, device)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_sup_loss = 0\n        train_accuracies = {\n            'supergroup': 0.0,\n            'group': 0.0,\n            'module': 0.0,\n            'brand': 0.0,\n            'item': 0.0\n        }\n\n        # Training loop\n        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            true_labels = {\n                'supergroup': batch['labels1'],\n                'group': batch['labels2'],\n                'module': batch['labels3'],\n                'brand': batch['labels4']\n            }\n\n            sup_loss = trainer.supervised_step(batch, true_labels)\n            total_sup_loss += sup_loss\n\n            # Compute training accuracies\n            with torch.no_grad():\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                batch_accuracies = trainer.compute_accuracy(outputs, true_labels)\n                for level in train_accuracies:\n                    train_accuracies[level] += batch_accuracies.get(level, 0.0)\n\n        # Calculate average training accuracies\n        for level in train_accuracies:\n            train_accuracies[level] /= len(train_loader)\n\n        # Validation loop\n        model.eval()\n        val_accuracies = {\n            'supergroup': 0.0,\n            'group': 0.0,\n            'module': 0.0,\n            'brand': 0.0,\n            'item': 0.0\n        }\n        val_loss = 0\n\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc='Validation'):\n                batch = {k: v.to(device) for k, v in batch.items()}\n                true_labels = {\n                    'supergroup': batch['labels1'],\n                    'group': batch['labels2'],\n                    'module': batch['labels3'],\n                    'brand': batch['labels4']\n                }\n\n                outputs = model(batch['input_ids'], batch['attention_mask'])\n                batch_accuracies = trainer.compute_accuracy(outputs, true_labels)\n                val_loss += trainer.compute_joint_loss(outputs, true_labels).item()\n\n                for level in val_accuracies:\n                    val_accuracies[level] += batch_accuracies.get(level, 0.0)\n\n        # Calculate average validation accuracies and loss\n        for level in val_accuracies:\n            val_accuracies[level] /= len(val_loader)\n        val_loss /= len(val_loader)\n\n        # Save best model based on average validation accuracy\n        avg_val_accuracy = sum(val_accuracies.values()) / len(val_accuracies)\n        trainer.save_best_model(avg_val_accuracy)\n        \n        # Print epoch results\n        print(f'\\nEpoch {epoch + 1} Results:')\n        print(f'Training Loss: {total_sup_loss / len(train_loader):.4f}')\n        print(f'Validation Loss: {val_loss:.4f}')\n        print('\\nTraining Accuracies:')\n        for level, acc in train_accuracies.items():\n            print(f'{level}: {acc:.4f}')\n        print('\\nValidation Accuracies:')\n        for level, acc in val_accuracies.items():\n            print(f'{level}: {acc:.4f}')\n        print('-' * 50)\n    \n    # Load the best model state at the end of training\n    if trainer.best_model_state is not None:\n        model.load_state_dict(trainer.best_model_state)\n\ntrain_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"papermill":{"duration":21328.964979,"end_time":"2024-10-26T18:27:53.469258","exception":false,"start_time":"2024-10-26T12:32:24.504279","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:09:17.149691Z","iopub.execute_input":"2024-10-27T08:09:17.150057Z","iopub.status.idle":"2024-10-27T08:12:14.481518Z","shell.execute_reply.started":"2024-10-27T08:09:17.150015Z","shell.execute_reply":"2024-10-27T08:12:14.479985Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e914e591bc2445449394e135742d8c9c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/34913142.py:164: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\nEpoch 1/10:   3%|▎         | 217/7023 [01:07<35:07,  3.23it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 89\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupergroup\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels1\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels2\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels3\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrand\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels4\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     87\u001b[0m }\n\u001b[0;32m---> 89\u001b[0m sup_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupervised_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m total_sup_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sup_loss\n","Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mJointAccuracyTrainer.supervised_step\u001b[0;34m(self, batch, true_labels)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupervised_optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 166\u001b[0m\n\u001b[1;32m    164\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Assuming you have train_loader and val_loader already defined\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 68\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     65\u001b[0m trainer \u001b[38;5;241m=\u001b[39m JointAccuracyTrainer(model, optimizer, device)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profile(activities\u001b[38;5;241m=\u001b[39m[ProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, ProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA],with_flops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m     69\u001b[0m         model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     70\u001b[0m         total_sup_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/profiler/profiler.py:706\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     prof\u001b[38;5;241m.\u001b[39mKinetoStepTracker\u001b[38;5;241m.\u001b[39merase_step_count(PROFILER_STEP_NAME)\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_trace_observer:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/profiler/profiler.py:722\u001b[0m, in \u001b[0;36mprofile.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn:\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transit_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/profiler/profiler.py:751\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[0;34m(self, prev_action, current_action)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_list:\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list:\n\u001b[0;32m--> 751\u001b[0m         \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/profiler/profiler.py:206\u001b[0m, in \u001b[0;36m_KinetoProfile.stop_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_trace_observer\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/profiler.py:352\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    347\u001b[0m t0 \u001b[38;5;241m=\u001b[39m perf_counter_ns()\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# TODO we are overwriting previous kineto results here\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Should combine previous results with the new results otherwise only\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# the last \"repeat\" will be recorded in the trace\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkineto_results \u001b[38;5;241m=\u001b[39m \u001b[43m_disable_profiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m t1 \u001b[38;5;241m=\u001b[39m perf_counter_ns()\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats\u001b[38;5;241m.\u001b[39mprofiler_disable_call_duration_us \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((t1 \u001b[38;5;241m-\u001b[39m t0) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n# Create the directory if it doesn't exist\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n# Save the model's state dictionary\n\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n# Save the tokenizer\n\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"papermill":{"duration":7.863377,"end_time":"2024-10-26T18:28:20.725111","exception":false,"start_time":"2024-10-26T18:28:12.861734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:12:14.482990Z","iopub.status.idle":"2024-10-27T08:12:14.483513Z","shell.execute_reply.started":"2024-10-27T08:12:14.483240Z","shell.execute_reply":"2024-10-27T08:12:14.483266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"papermill":{"duration":7.514779,"end_time":"2024-10-26T18:28:34.708312","exception":false,"start_time":"2024-10-26T18:28:27.193533","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:13:33.493984Z","iopub.execute_input":"2024-10-27T08:13:33.494707Z","iopub.status.idle":"2024-10-27T08:13:34.349817Z","shell.execute_reply.started":"2024-10-27T08:13:33.494664Z","shell.execute_reply":"2024-10-27T08:13:34.348742Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df_test_feat.head()","metadata":{"papermill":{"duration":6.550644,"end_time":"2024-10-26T18:28:47.683962","exception":false,"start_time":"2024-10-26T18:28:41.133318","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:13:34.351740Z","iopub.execute_input":"2024-10-27T08:13:34.352065Z","iopub.status.idle":"2024-10-27T08:13:34.367543Z","shell.execute_reply.started":"2024-10-27T08:13:34.352032Z","shell.execute_reply":"2024-10-27T08:13:34.366654Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   indoml_id                    description retailer  price\n0          0             14 in hybrid blade    wilko   4.50\n1          1         2 pk vent stick a fres  noshify   0.69\n2          2               4 tyrefix 450 ml  noshify   2.99\n3          3           4 x 4 tyrefix 450 ml  noshify   2.99\n4          4  5 l adbluescr diesel vehicles  noshify   4.99","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>description</th>\n      <th>retailer</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>14 in hybrid blade</td>\n      <td>wilko</td>\n      <td>4.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2 pk vent stick a fres</td>\n      <td>noshify</td>\n      <td>0.69</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>4 tyrefix 450 ml</td>\n      <td>noshify</td>\n      <td>2.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4 x 4 tyrefix 450 ml</td>\n      <td>noshify</td>\n      <td>2.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5 l adbluescr diesel vehicles</td>\n      <td>noshify</td>\n      <td>4.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def predict(model, tokenizer, text):\n    inputs = tokenizer(\n        text, \n        return_tensors=\"pt\", \n        truncation=True, \n        padding='max_length', \n        max_length=MAX_LENGTH\n    ).to(DEVICE)\n    with torch.no_grad():\n        start = time.time()\n        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n\n            # Adjust outputs if it's a tuple or dictionary\n            if isinstance(outputs, tuple):\n                logits = outputs[0]  # Assuming the first element contains logits\n            elif isinstance(outputs, dict):\n                logits = list(outputs.values())  # Assuming values in dict are tensors\n            else:\n                logits = outputs  # If it's already a tensor list\n\n            # Generate predictions\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n\n        print(\"Inference time :\"+str(time.time()-start))\n        #print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n        print(\"GFLOPs during testing\") #GigaFLOPs\n        print(sum(k.flops for k in prof.key_averages())/1e9)\n    \n    return predictions\n\n# Example prediction\nsample_text = \"14 in hybrid blade\"\npredictions = predict(model, tokenizer, sample_text)\nprint(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")","metadata":{"papermill":{"duration":7.059777,"end_time":"2024-10-26T18:29:01.395181","exception":true,"start_time":"2024-10-26T18:28:54.335404","status":"failed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T10:07:29.414986Z","iopub.execute_input":"2024-10-27T10:07:29.415843Z","iopub.status.idle":"2024-10-27T10:07:30.184401Z","shell.execute_reply.started":"2024-10-27T10:07:29.415801Z","shell.execute_reply":"2024-10-27T10:07:30.183474Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Inference time :0.5156702995300293\nGFLOPs during testing\n16.781984352\nSupergroup: 16, Group: 114, Module: 212, Brand: 4078\n","output_type":"stream"}]},{"cell_type":"code","source":"# test_tmp = df_test_feat[:5]","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:13:34.653173Z","iopub.execute_input":"2024-10-27T08:13:34.654015Z","iopub.status.idle":"2024-10-27T08:13:34.657547Z","shell.execute_reply.started":"2024-10-27T08:13:34.653975Z","shell.execute_reply":"2024-10-27T08:13:34.656722Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    \n    with torch.no_grad():\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n\n            # Tokenize input\n            inputs = tokenizer(\n                df_test_feat.iloc[i].description, \n                return_tensors=\"pt\", \n                truncation=True, \n                padding='max_length', \n                max_length=MAX_LENGTH\n            ).to(DEVICE)\n\n            # Get model outputs\n            outputs = model(inputs['input_ids'], inputs['attention_mask'])\n\n            # Adjust outputs if it's a tuple or dictionary\n            if isinstance(outputs, tuple):\n                logits = outputs[0]  # Assuming the first element contains logits\n            elif isinstance(outputs, dict):\n                logits = list(outputs.values())  # Assuming values in dict are tensors\n            else:\n                logits = outputs  # If it's already a tensor list\n\n            # Generate predictions\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n\n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n            \n    # Map predictions back to names using encoders\n    try:\n        supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n    except ValueError as e:\n        print(f\"Error in supergroups: {e}\")\n        supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n    \n    try:\n        groups_names = group_encoder.inverse_transform(groups_list)\n    except ValueError as e:\n        print(f\"Error in groups: {e}\")\n        groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n    \n    try:\n        modules_names = module_encoder.inverse_transform(modules_list)\n    except ValueError as e:\n        print(f\"Error in modules: {e}\")\n        modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n    \n    try:\n        brands_names = brand_encoder.inverse_transform(brands_list)\n    except ValueError as e:\n        print(f\"Error in brands: {e}\")\n        brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n    \n    # Create a DataFrame with predictions\n    predictions_df = pd.DataFrame({\n        'indoml_id': indoml_id_list,\n        'supergroup': supergroups_names,\n        'group': groups_names,\n        'module': modules_names,\n        'brand': brands_names\n    })\n    \n    # Save the predictions as a JSON file\n    predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n    print(\"predictions.predict saved\")\n        \nprint(make_test_pred_and_save(df_test_feat))","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T08:21:48.285836Z","iopub.execute_input":"2024-10-27T08:21:48.286335Z","iopub.status.idle":"2024-10-27T10:05:37.199863Z","shell.execute_reply.started":"2024-10-27T08:21:48.286286Z","shell.execute_reply":"2024-10-27T10:05:37.198771Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Processing 0 of 184663\nProcessing 1000 of 184663\nProcessing 2000 of 184663\nProcessing 3000 of 184663\nProcessing 4000 of 184663\nProcessing 5000 of 184663\nProcessing 6000 of 184663\nProcessing 7000 of 184663\nProcessing 8000 of 184663\nProcessing 9000 of 184663\nProcessing 10000 of 184663\nProcessing 11000 of 184663\nProcessing 12000 of 184663\nProcessing 13000 of 184663\nProcessing 14000 of 184663\nProcessing 15000 of 184663\nProcessing 16000 of 184663\nProcessing 17000 of 184663\nProcessing 18000 of 184663\nProcessing 19000 of 184663\nProcessing 20000 of 184663\nProcessing 21000 of 184663\nProcessing 22000 of 184663\nProcessing 23000 of 184663\nProcessing 24000 of 184663\nProcessing 25000 of 184663\nProcessing 26000 of 184663\nProcessing 27000 of 184663\nProcessing 28000 of 184663\nProcessing 29000 of 184663\nProcessing 30000 of 184663\nProcessing 31000 of 184663\nProcessing 32000 of 184663\nProcessing 33000 of 184663\nProcessing 34000 of 184663\nProcessing 35000 of 184663\nProcessing 36000 of 184663\nProcessing 37000 of 184663\nProcessing 38000 of 184663\nProcessing 39000 of 184663\nProcessing 40000 of 184663\nProcessing 41000 of 184663\nProcessing 45000 of 184663\nProcessing 46000 of 184663\nProcessing 47000 of 184663\nProcessing 48000 of 184663\nProcessing 49000 of 184663\nProcessing 50000 of 184663\nProcessing 51000 of 184663\nProcessing 52000 of 184663\nProcessing 53000 of 184663\nProcessing 54000 of 184663\nProcessing 55000 of 184663\nProcessing 56000 of 184663\nProcessing 57000 of 184663\nProcessing 58000 of 184663\nProcessing 59000 of 184663\nProcessing 60000 of 184663\nProcessing 61000 of 184663\nProcessing 62000 of 184663\nProcessing 63000 of 184663\nProcessing 64000 of 184663\nProcessing 65000 of 184663\nProcessing 66000 of 184663\nProcessing 67000 of 184663\nProcessing 68000 of 184663\nProcessing 69000 of 184663\nProcessing 70000 of 184663\nProcessing 71000 of 184663\nProcessing 72000 of 184663\nProcessing 73000 of 184663\nProcessing 74000 of 184663\nProcessing 75000 of 184663\nProcessing 76000 of 184663\nProcessing 77000 of 184663\nProcessing 78000 of 184663\nProcessing 79000 of 184663\nProcessing 80000 of 184663\nProcessing 81000 of 184663\nProcessing 82000 of 184663\nProcessing 83000 of 184663\nProcessing 84000 of 184663\nProcessing 85000 of 184663\nProcessing 86000 of 184663\nProcessing 87000 of 184663\nProcessing 88000 of 184663\nProcessing 89000 of 184663\nProcessing 90000 of 184663\nProcessing 91000 of 184663\nProcessing 92000 of 184663\nProcessing 97000 of 184663\nProcessing 98000 of 184663\nProcessing 99000 of 184663\nProcessing 100000 of 184663\nProcessing 101000 of 184663\nProcessing 102000 of 184663\nProcessing 103000 of 184663\nProcessing 104000 of 184663\nProcessing 105000 of 184663\nProcessing 106000 of 184663\nProcessing 107000 of 184663\nProcessing 108000 of 184663\nProcessing 109000 of 184663\nProcessing 110000 of 184663\nProcessing 111000 of 184663\nProcessing 112000 of 184663\nProcessing 113000 of 184663\nProcessing 114000 of 184663\nProcessing 115000 of 184663\nProcessing 116000 of 184663\nProcessing 117000 of 184663\nProcessing 118000 of 184663\nProcessing 119000 of 184663\nProcessing 120000 of 184663\nProcessing 121000 of 184663\nProcessing 122000 of 184663\nProcessing 123000 of 184663\nProcessing 124000 of 184663\nProcessing 125000 of 184663\nProcessing 126000 of 184663\nProcessing 127000 of 184663\nProcessing 128000 of 184663\nProcessing 129000 of 184663\nProcessing 130000 of 184663\nProcessing 131000 of 184663\nProcessing 132000 of 184663\nProcessing 133000 of 184663\nProcessing 134000 of 184663\nProcessing 135000 of 184663\nProcessing 136000 of 184663\nProcessing 137000 of 184663\nProcessing 138000 of 184663\nProcessing 139000 of 184663\nProcessing 140000 of 184663\nProcessing 141000 of 184663\nProcessing 142000 of 184663\nProcessing 143000 of 184663\nProcessing 144000 of 184663\nProcessing 145000 of 184663\nProcessing 146000 of 184663\nProcessing 147000 of 184663\nProcessing 148000 of 184663\nProcessing 149000 of 184663\nProcessing 150000 of 184663\nProcessing 151000 of 184663\nProcessing 152000 of 184663\nProcessing 153000 of 184663\nProcessing 154000 of 184663\nProcessing 155000 of 184663\nProcessing 156000 of 184663\nProcessing 157000 of 184663\nProcessing 158000 of 184663\nProcessing 159000 of 184663\nProcessing 160000 of 184663\nProcessing 161000 of 184663\nProcessing 162000 of 184663\nProcessing 163000 of 184663\nProcessing 164000 of 184663\nProcessing 165000 of 184663\nProcessing 166000 of 184663\nProcessing 167000 of 184663\nProcessing 168000 of 184663\nProcessing 169000 of 184663\nProcessing 170000 of 184663\nProcessing 171000 of 184663\nProcessing 172000 of 184663\nProcessing 173000 of 184663\nProcessing 174000 of 184663\nProcessing 175000 of 184663\nProcessing 176000 of 184663\nProcessing 177000 of 184663\nProcessing 178000 of 184663\nProcessing 179000 of 184663\nProcessing 180000 of 184663\nProcessing 181000 of 184663\nProcessing 182000 of 184663\nProcessing 183000 of 184663\nProcessing 184000 of 184663\npredictions.predict saved\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[]},"execution_count":null,"outputs":[]}]}