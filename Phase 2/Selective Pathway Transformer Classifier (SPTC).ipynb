{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9637413,"sourceType":"datasetVersion","datasetId":5884412},{"sourceId":202281922,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndf_features = pd.read_json('/kaggle/input/new-codalab-final-test/training_data/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/new-codalab-final-test/training_data/train.labels',lines=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T10:05:31.681666Z","iopub.execute_input":"2024-10-27T10:05:31.682314Z","iopub.status.idle":"2024-10-27T10:05:38.746668Z","shell.execute_reply.started":"2024-10-27T10:05:31.682278Z","shell.execute_reply":"2024-10-27T10:05:38.745622Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:38.749032Z","iopub.execute_input":"2024-10-27T10:05:38.749843Z","iopub.status.idle":"2024-10-27T10:05:38.900024Z","shell.execute_reply.started":"2024-10-27T10:05:38.749794Z","shell.execute_reply":"2024-10-27T10:05:38.899169Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:38.901061Z","iopub.execute_input":"2024-10-27T10:05:38.901360Z","iopub.status.idle":"2024-10-27T10:05:38.924512Z","shell.execute_reply.started":"2024-10-27T10:05:38.901329Z","shell.execute_reply":"2024-10-27T10:05:38.923754Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        indoml_id                 description      retailer  price  \\\n0               0                    1 adblue  organicorner  25.35   \n1               1               1 car mat set   greenharbor   4.99   \n2               2           1 cp rmx scrnwash      naturify   3.85   \n3               3                    1 diesel        ecogro   4.41   \n4               4      1 unstoppable refrsher   greenharbor   3.00   \n...           ...                         ...           ...    ...   \n561833     561833       zuru xshot excelxcess       noshify  16.99   \n561834     561834            zuru xshot micro      vitalveg   3.50   \n561835     561835  zuru xshot typhoon thunder   crispcorner   8.50   \n561836     561836                       zzand      snackify   4.79   \n561837     561837   zzand sand set dinounicon       noshify   6.99   \n\n        supergroup                            group      module        brand  \n0       automotive  automotive detail unknown total  automotive  receipt all  \n1       automotive  automotive detail unknown total  automotive  receipt all  \n2       automotive  automotive detail unknown total  automotive  receipt all  \n3       automotive  automotive detail unknown total  automotive  receipt all  \n4       automotive  automotive detail unknown total  automotive  receipt all  \n...            ...                              ...         ...          ...  \n561833        toys        toys detail unknown total        toys  receipt all  \n561834        toys        toys detail unknown total        toys  receipt all  \n561835        toys        toys detail unknown total        toys  receipt all  \n561836        toys        toys detail unknown total        toys  receipt all  \n561837        toys        toys detail unknown total        toys  receipt all  \n\n[561838 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>description</th>\n      <th>retailer</th>\n      <th>price</th>\n      <th>supergroup</th>\n      <th>group</th>\n      <th>module</th>\n      <th>brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1 adblue</td>\n      <td>organicorner</td>\n      <td>25.35</td>\n      <td>automotive</td>\n      <td>automotive detail unknown total</td>\n      <td>automotive</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1 car mat set</td>\n      <td>greenharbor</td>\n      <td>4.99</td>\n      <td>automotive</td>\n      <td>automotive detail unknown total</td>\n      <td>automotive</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1 cp rmx scrnwash</td>\n      <td>naturify</td>\n      <td>3.85</td>\n      <td>automotive</td>\n      <td>automotive detail unknown total</td>\n      <td>automotive</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1 diesel</td>\n      <td>ecogro</td>\n      <td>4.41</td>\n      <td>automotive</td>\n      <td>automotive detail unknown total</td>\n      <td>automotive</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1 unstoppable refrsher</td>\n      <td>greenharbor</td>\n      <td>3.00</td>\n      <td>automotive</td>\n      <td>automotive detail unknown total</td>\n      <td>automotive</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>561833</th>\n      <td>561833</td>\n      <td>zuru xshot excelxcess</td>\n      <td>noshify</td>\n      <td>16.99</td>\n      <td>toys</td>\n      <td>toys detail unknown total</td>\n      <td>toys</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>561834</th>\n      <td>561834</td>\n      <td>zuru xshot micro</td>\n      <td>vitalveg</td>\n      <td>3.50</td>\n      <td>toys</td>\n      <td>toys detail unknown total</td>\n      <td>toys</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>561835</th>\n      <td>561835</td>\n      <td>zuru xshot typhoon thunder</td>\n      <td>crispcorner</td>\n      <td>8.50</td>\n      <td>toys</td>\n      <td>toys detail unknown total</td>\n      <td>toys</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>561836</th>\n      <td>561836</td>\n      <td>zzand</td>\n      <td>snackify</td>\n      <td>4.79</td>\n      <td>toys</td>\n      <td>toys detail unknown total</td>\n      <td>toys</td>\n      <td>receipt all</td>\n    </tr>\n    <tr>\n      <th>561837</th>\n      <td>561837</td>\n      <td>zzand sand set dinounicon</td>\n      <td>noshify</td>\n      <td>6.99</td>\n      <td>toys</td>\n      <td>toys detail unknown total</td>\n      <td>toys</td>\n      <td>receipt all</td>\n    </tr>\n  </tbody>\n</table>\n<p>561838 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df[\"brand\"].unique())","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:38.926350Z","iopub.execute_input":"2024-10-27T10:05:38.926642Z","iopub.status.idle":"2024-10-27T10:05:39.035391Z","shell.execute_reply.started":"2024-10-27T10:05:38.926611Z","shell.execute_reply":"2024-10-27T10:05:39.034456Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"5679"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])\n\ndf['description'] = df['description'] + \" \" + df['retailer'].astype(str) +\" \" +df[\"price\"].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:39.036637Z","iopub.execute_input":"2024-10-27T10:05:39.037043Z","iopub.status.idle":"2024-10-27T10:05:41.135720Z","shell.execute_reply.started":"2024-10-27T10:05:39.036997Z","shell.execute_reply":"2024-10-27T10:05:41.134684Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"brand_encoder.classes_","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:41.136970Z","iopub.execute_input":"2024-10-27T10:05:41.137542Z","iopub.status.idle":"2024-10-27T10:05:41.144130Z","shell.execute_reply.started":"2024-10-27T10:05:41.137496Z","shell.execute_reply":"2024-10-27T10:05:41.143127Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['& soda', '10 motives', '1001', ..., 'zumi', 'zylkene', 'zyn'],\n      dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:41.145330Z","iopub.execute_input":"2024-10-27T10:05:41.145690Z","iopub.status.idle":"2024-10-27T10:05:41.164039Z","shell.execute_reply.started":"2024-10-27T10:05:41.145649Z","shell.execute_reply":"2024-10-27T10:05:41.163144Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"        indoml_id                                 description      retailer  \\\n0               0                 1 adblue organicorner 25.35  organicorner   \n1               1              1 car mat set greenharbor 4.99   greenharbor   \n2               2             1 cp rmx scrnwash naturify 3.85      naturify   \n3               3                        1 diesel ecogro 4.41        ecogro   \n4               4      1 unstoppable refrsher greenharbor 3.0   greenharbor   \n...           ...                                         ...           ...   \n561833     561833         zuru xshot excelxcess noshify 16.99       noshify   \n561834     561834               zuru xshot micro vitalveg 3.5      vitalveg   \n561835     561835  zuru xshot typhoon thunder crispcorner 8.5   crispcorner   \n561836     561836                         zzand snackify 4.79      snackify   \n561837     561837      zzand sand set dinounicon noshify 6.99       noshify   \n\n        price  supergroup  group  module  brand  \n0       25.35           0      2      18   4078  \n1        4.99           0      2      18   4078  \n2        3.85           0      2      18   4078  \n3        4.41           0      2      18   4078  \n4        3.00           0      2      18   4078  \n...       ...         ...    ...     ...    ...  \n561833  16.99          31    215     407   4078  \n561834   3.50          31    215     407   4078  \n561835   8.50          31    215     407   4078  \n561836   4.79          31    215     407   4078  \n561837   6.99          31    215     407   4078  \n\n[561838 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>description</th>\n      <th>retailer</th>\n      <th>price</th>\n      <th>supergroup</th>\n      <th>group</th>\n      <th>module</th>\n      <th>brand</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1 adblue organicorner 25.35</td>\n      <td>organicorner</td>\n      <td>25.35</td>\n      <td>0</td>\n      <td>2</td>\n      <td>18</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1 car mat set greenharbor 4.99</td>\n      <td>greenharbor</td>\n      <td>4.99</td>\n      <td>0</td>\n      <td>2</td>\n      <td>18</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1 cp rmx scrnwash naturify 3.85</td>\n      <td>naturify</td>\n      <td>3.85</td>\n      <td>0</td>\n      <td>2</td>\n      <td>18</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1 diesel ecogro 4.41</td>\n      <td>ecogro</td>\n      <td>4.41</td>\n      <td>0</td>\n      <td>2</td>\n      <td>18</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1 unstoppable refrsher greenharbor 3.0</td>\n      <td>greenharbor</td>\n      <td>3.00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>18</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>561833</th>\n      <td>561833</td>\n      <td>zuru xshot excelxcess noshify 16.99</td>\n      <td>noshify</td>\n      <td>16.99</td>\n      <td>31</td>\n      <td>215</td>\n      <td>407</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>561834</th>\n      <td>561834</td>\n      <td>zuru xshot micro vitalveg 3.5</td>\n      <td>vitalveg</td>\n      <td>3.50</td>\n      <td>31</td>\n      <td>215</td>\n      <td>407</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>561835</th>\n      <td>561835</td>\n      <td>zuru xshot typhoon thunder crispcorner 8.5</td>\n      <td>crispcorner</td>\n      <td>8.50</td>\n      <td>31</td>\n      <td>215</td>\n      <td>407</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>561836</th>\n      <td>561836</td>\n      <td>zzand snackify 4.79</td>\n      <td>snackify</td>\n      <td>4.79</td>\n      <td>31</td>\n      <td>215</td>\n      <td>407</td>\n      <td>4078</td>\n    </tr>\n    <tr>\n      <th>561837</th>\n      <td>561837</td>\n      <td>zzand sand set dinounicon noshify 6.99</td>\n      <td>noshify</td>\n      <td>6.99</td>\n      <td>31</td>\n      <td>215</td>\n      <td>407</td>\n      <td>4078</td>\n    </tr>\n  </tbody>\n</table>\n<p>561838 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"supergroup_dict = {}\nfor val in df[\"supergroup\"].unique():\n    new_df = df[df[\"supergroup\"]==val]\n    supergroup_dict[val] = new_df[\"group\"].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:41.165155Z","iopub.execute_input":"2024-10-27T10:05:41.165459Z","iopub.status.idle":"2024-10-27T10:05:41.269364Z","shell.execute_reply.started":"2024-10-27T10:05:41.165418Z","shell.execute_reply":"2024-10-27T10:05:41.268466Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"group_dict = {}\nfor val in df[\"group\"].unique():\n    new_df = df[df[\"group\"]==val]\n    group_dict[val] = new_df[\"module\"].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:41.270531Z","iopub.execute_input":"2024-10-27T10:05:41.270840Z","iopub.status.idle":"2024-10-27T10:05:41.562067Z","shell.execute_reply.started":"2024-10-27T10:05:41.270807Z","shell.execute_reply":"2024-10-27T10:05:41.561225Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"module_dict = {}\nfor val in df[\"module\"].unique():\n    new_df = df[df[\"module\"]==val]\n    module_dict[val] = new_df[\"brand\"].unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:41.565030Z","iopub.execute_input":"2024-10-27T10:05:41.565366Z","iopub.status.idle":"2024-10-27T10:05:42.070839Z","shell.execute_reply.started":"2024-10-27T10:05:41.565324Z","shell.execute_reply":"2024-10-27T10:05:42.070030Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\n# from transformers import DistilBertTokenizer, DistilBertModel\nfrom transformers import BertForSequenceClassification\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.amp import autocast, GradScaler\n\n# Constants\nMAX_LENGTH = 20\nBATCH_SIZE = 32\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 0\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:42.072147Z","iopub.execute_input":"2024-10-27T10:05:42.072515Z","iopub.status.idle":"2024-10-27T10:05:47.234609Z","shell.execute_reply.started":"2024-10-27T10:05:42.072470Z","shell.execute_reply":"2024-10-27T10:05:47.233793Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!pip install torch_optimizer","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:05:47.235868Z","iopub.execute_input":"2024-10-27T10:05:47.236468Z","iopub.status.idle":"2024-10-27T10:06:00.116778Z","shell.execute_reply.started":"2024-10-27T10:05:47.236419Z","shell.execute_reply":"2024-10-27T10:06:00.115743Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting torch_optimizer\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from torch_optimizer) (2.4.0)\nCollecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.5.0->torch_optimizer) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.5.0->torch_optimizer) (1.3.0)\nDownloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nInstalling collected packages: pytorch-ranger, torch_optimizer\nSuccessfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nfrom transformers import AutoModel, AutoTokenizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch_optimizer import AdamP\n\n# Constants (keep the existing constants like MAX_LENGTH, BATCH_SIZE, etc.)\n\ntokenizer = AutoTokenizer.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")\n\n\ndef custom_hierarchical_loss(logits, labels, valid_indices):\n    batch_size = logits.size(0)\n    loss = 0\n    for i in range(batch_size):\n        valid_idx = torch.tensor(list(valid_indices[i]), device=logits.device)\n        if valid_idx.numel() == 0:\n            continue\n        valid_logits = logits[i, valid_idx]\n        label = labels[i]\n        if label not in valid_idx:\n            label = valid_idx[0]\n        label_idx = torch.where(valid_idx == label)[0]\n        if label_idx.numel() == 0:\n            continue\n        loss += nn.CrossEntropyLoss()(valid_logits.unsqueeze(0), label_idx)\n    return loss / batch_size if batch_size > 0 else loss\n\n# Separate Model for each level of hierarchy\nclass SupergroupModel(nn.Module):\n    def __init__(self, num_supergroups):\n        super(SupergroupModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")\n        self.supergroup_classifier = nn.Linear(self.base_model.config.hidden_size, num_supergroups)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0, :]\n        return self.supergroup_classifier(pooled_output)\n\nclass GroupModel(nn.Module):\n    def __init__(self, num_groups):\n        super(GroupModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")\n        self.group_classifier = nn.Linear(self.base_model.config.hidden_size, num_groups)\n        self.supergroup_to_group = supergroup_dict  # Hierarchical mapping\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0, :]\n        return self.group_classifier(pooled_output)\n\n    def predict(self, input_ids, attention_mask, supergroup_preds):\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        group_probs = torch.softmax(logits, dim=1)\n        \n        group_preds = []\n        batch_size = supergroup_preds.size(0)\n\n        for i in range(batch_size):\n            supergroup = supergroup_preds[i].item()\n            valid_groups = self.supergroup_to_group.get(supergroup, range(logits.size(1)))\n            valid_group_probs = group_probs[i, valid_groups]\n            group_pred = valid_groups[torch.argmax(valid_group_probs).item()]\n            group_preds.append(group_pred)\n\n        return torch.tensor(group_preds).to(DEVICE)\n\nclass ModuleModel(nn.Module):\n    def __init__(self, num_modules):\n        super(ModuleModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")\n        self.module_classifier = nn.Linear(self.base_model.config.hidden_size, num_modules)\n        self.group_to_module = group_dict  # Hierarchical mapping\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0, :]\n        return self.module_classifier(pooled_output)\n\n    def predict(self, input_ids, attention_mask, group_preds):\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        module_probs = torch.softmax(logits, dim=1)\n        \n        module_preds = []\n        batch_size = group_preds.size(0)\n\n        for i in range(batch_size):\n            group = group_preds[i].item()\n            valid_modules = self.group_to_module.get(group, range(logits.size(1)))\n            valid_module_probs = module_probs[i, valid_modules]\n            module_pred = valid_modules[torch.argmax(valid_module_probs).item()]\n            module_preds.append(module_pred)\n\n        return torch.tensor(module_preds).to(DEVICE)\n\nclass BrandModel(nn.Module):\n    def __init__(self, num_brands):\n        super(BrandModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")\n        self.brand_classifier = nn.Linear(self.base_model.config.hidden_size, num_brands)\n        self.module_to_brand = module_dict  # Hierarchical mapping from module to brand\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs[0][:, 0, :]\n        return self.brand_classifier(pooled_output)\n\n    def predict(self, input_ids, attention_mask, module_preds):\n        logits = self(input_ids=input_ids, attention_mask=attention_mask)\n        brand_probs = torch.softmax(logits, dim=1)\n\n        brand_preds = []\n        batch_size = module_preds.size(0)\n\n        for i in range(batch_size):\n            module = module_preds[i].item()\n            valid_brands = self.module_to_brand.get(module, range(logits.size(1)))\n            valid_brand_probs = brand_probs[i, valid_brands]\n            brand_pred = valid_brands[torch.argmax(valid_brand_probs).item()]\n            brand_preds.append(brand_pred)\n\n        return torch.tensor(brand_preds).to(DEVICE)\n\n\n# Dataset class remains the same\nclass ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1  # Supergroup\n        self.labels2 = labels2  # Group\n        self.labels3 = labels3  # Module\n        self.labels4 = labels4  # Brand\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n\n\n\n# Training and evaluation logic for separate models\ndef compute_accuracy(preds, labels):\n    return accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n\n\n# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3 = train_test_split(\n    df['description'], df['supergroup'], df['group'], df['module'], test_size=0.2, random_state=42\n)\n\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], df['supergroup'], df['group'], df['module'], df['brand'], test_size=0.2, random_state=42\n)\n\ntrain_dataset = ProductDataset(train_texts.tolist(), train_labels1.tolist(), train_labels2.tolist(), train_labels3.tolist(), train_labels4.tolist())\nval_dataset = ProductDataset(val_texts.tolist(), val_labels1.tolist(), val_labels2.tolist(), val_labels3.tolist(), val_labels4.tolist())\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n\n# Initialize separate models\nsupergroup_model = torch.load('/kaggle/input/new-indoml-model/supergroup_model.pth').to(DEVICE)\ngroup_model = torch.load('/kaggle/input/new-indoml-model/group_model.pth').to(DEVICE)\nmodule_model = torch.load('/kaggle/input/new-indoml-model/module_model.pth').to(DEVICE)\nbrand_model = torch.load('/kaggle/input/new-indoml-model/brand_model.pth').to(DEVICE)\n\noptimizer_supergroup = AdamP(supergroup_model.parameters(), lr=LEARNING_RATE)\noptimizer_group = AdamP(group_model.parameters(), lr=LEARNING_RATE)\noptimizer_module = AdamP(module_model.parameters(), lr=LEARNING_RATE)\noptimizer_brand = AdamP(brand_model.parameters(), lr=LEARNING_RATE)\n\nscaler = GradScaler()\nscheduler_supergroup = ReduceLROnPlateau(optimizer_supergroup, mode='min', patience=PATIENCE_LR, factor=0.5, verbose=True)\nscheduler_group = ReduceLROnPlateau(optimizer_group, mode='min', patience=PATIENCE_LR, factor=0.5, verbose=True)\nscheduler_module = ReduceLROnPlateau(optimizer_module, mode='min', patience=PATIENCE_LR, factor=0.5, verbose=True)\nscheduler_brand = ReduceLROnPlateau(optimizer_brand, mode='min', patience=PATIENCE_LR, factor=0.5, verbose=True)\n\n\ndef validate(supergroup_model, group_model, module_model, brand_model, val_loader):\n    supergroup_model.eval()\n    group_model.eval()\n    module_model.eval()\n    brand_model.eval()\n\n    total_val_loss = 0\n    total_val_acc1, total_val_acc2, total_val_acc3, total_val_acc4 = 0, 0, 0, 0\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=\"Validating\"):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels1, labels2, labels3, labels4 = batch['labels1'], batch['labels2'], batch['labels3'], batch['labels4']\n\n            # Supergroup prediction\n            logits1 = supergroup_model(input_ids=input_ids, attention_mask=attention_mask)\n            loss1 = nn.CrossEntropyLoss()(logits1, labels1)\n            supergroup_preds = torch.argmax(torch.softmax(logits1, dim=1), dim=1)\n\n            # Group prediction\n            logits2 = group_model(input_ids=input_ids, attention_mask=attention_mask)\n            loss2 = nn.CrossEntropyLoss()(logits2, labels2)\n            group_preds = group_model.predict(input_ids=input_ids, attention_mask=attention_mask, supergroup_preds=supergroup_preds)\n\n            # Module prediction\n            logits3 = module_model(input_ids=input_ids, attention_mask=attention_mask)\n            loss3 = nn.CrossEntropyLoss()(logits3, labels3)\n            module_preds = module_model.predict(input_ids=input_ids, attention_mask=attention_mask, group_preds=group_preds)\n\n            # Brand prediction\n            logits4 = brand_model(input_ids=input_ids, attention_mask=attention_mask)\n            loss4 = nn.CrossEntropyLoss()(logits4, labels4)\n            brand_preds = brand_model.predict(input_ids=input_ids, attention_mask=attention_mask, module_preds=module_preds)\n\n            # Calculate total loss and accuracy for validation\n            total_val_loss += (loss1.item() + loss2.item() + loss3.item() + loss4.item())\n            total_val_acc1 += compute_accuracy(supergroup_preds, labels1)\n            total_val_acc2 += compute_accuracy(group_preds, labels2)\n            total_val_acc3 += compute_accuracy(module_preds, labels3)\n            total_val_acc4 += compute_accuracy(brand_preds, labels4)\n\n    avg_val_loss = total_val_loss / len(val_loader)\n    avg_val_acc1 = total_val_acc1 / len(val_loader)\n    avg_val_acc2 = total_val_acc2 / len(val_loader)\n    avg_val_acc3 = total_val_acc3 / len(val_loader)\n    avg_val_acc4 = total_val_acc4 / len(val_loader)\n\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n    print(f\"Supergroup Accuracy: {avg_val_acc1:.4f}\")\n    print(f\"Group Accuracy: {avg_val_acc2:.4f}\")\n    print(f\"Module Accuracy: {avg_val_acc3:.4f}\")\n    print(f\"Brand Accuracy: {avg_val_acc4:.4f}\")\n\n    return avg_val_loss, avg_val_acc1, avg_val_acc2, avg_val_acc3, avg_val_acc4\n\n\n\n# Training loop\n# Training loop (now with brand model)\nfor epoch in range(NUM_EPOCHS):\n    supergroup_model.train()\n    group_model.train()\n    module_model.train()\n    brand_model.train()\n    \n    total_train_loss = 0\n    total_train_acc1, total_train_acc2, total_train_acc3, total_train_acc4 = 0, 0, 0, 0\n\n    with tqdm(total=len(train_loader), desc=f'Training Epoch {epoch+1}/{NUM_EPOCHS}', unit='batch', leave=False) as pbar:\n        for batch in train_loader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels1, labels2, labels3, labels4 = batch['labels1'], batch['labels2'], batch['labels3'], batch['labels4']\n\n            # Supergroup model\n            with torch.amp.autocast('cuda'):\n                logits1 = supergroup_model(input_ids=input_ids, attention_mask=attention_mask)\n                loss1 = nn.CrossEntropyLoss()(logits1, labels1)\n            scaler.scale(loss1).backward()\n            scaler.step(optimizer_supergroup)\n            scaler.update()\n            optimizer_supergroup.zero_grad()\n\n            supergroup_preds = torch.argmax(torch.softmax(logits1, dim=1), dim=1)\n\n            # Group model\n            with torch.amp.autocast('cuda'):\n                logits2 = group_model(input_ids=input_ids, attention_mask=attention_mask)\n                loss2 = nn.CrossEntropyLoss()(logits2, labels2)\n            scaler.scale(loss2).backward()\n            scaler.step(optimizer_group)\n            scaler.update()\n            optimizer_group.zero_grad()\n\n            group_preds = group_model.predict(input_ids=input_ids, attention_mask=attention_mask, supergroup_preds=supergroup_preds)\n\n            # Module model\n            with torch.amp.autocast('cuda'):\n                logits3 = module_model(input_ids=input_ids, attention_mask=attention_mask)\n                loss3 = nn.CrossEntropyLoss()(logits3, labels3)\n            scaler.scale(loss3).backward()\n            scaler.step(optimizer_module)\n            scaler.update()\n            optimizer_module.zero_grad()\n\n            module_preds = module_model.predict(input_ids=input_ids, attention_mask=attention_mask, group_preds=group_preds)\n\n            # Brand model\n            with torch.amp.autocast('cuda'):\n                logits4 = brand_model(input_ids=input_ids, attention_mask=attention_mask)\n                loss4 = nn.CrossEntropyLoss()(logits4, labels4)\n            scaler.scale(loss4).backward()\n            scaler.step(optimizer_brand)\n            scaler.update()\n            optimizer_brand.zero_grad()\n\n            brand_preds = brand_model.predict(input_ids=input_ids, attention_mask=attention_mask, module_preds=module_preds)\n\n            total_train_loss += (loss1.item() + loss2.item() + loss3.item() + loss4.item())\n            total_train_acc1 += compute_accuracy(supergroup_preds, labels1)\n            total_train_acc2 += compute_accuracy(group_preds, labels2)\n            total_train_acc3 += compute_accuracy(module_preds, labels3)\n            total_train_acc4 += compute_accuracy(brand_preds, labels4)\n\n            pbar.update(1)\n\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_train_acc1 = total_train_acc1 / len(train_loader)\n    avg_train_acc2 = total_train_acc2 / len(train_loader)\n    avg_train_acc3 = total_train_acc3 / len(train_loader)\n    avg_train_acc4 = total_train_acc4 / len(train_loader)\n\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {avg_train_loss:.4f}\")\n    print(f\"Supergroup Accuracy: {avg_train_acc1:.4f}\")\n    print(f\"Group Accuracy: {avg_train_acc2:.4f}\")\n    print(f\"Module Accuracy: {avg_train_acc3:.4f}\")\n    print(f\"Brand Accuracy: {avg_train_acc4:.4f}\")\n\n    # Validation step\n    avg_val_loss, avg_val_acc1, avg_val_acc2, avg_val_acc3, avg_val_acc4 = validate(supergroup_model, group_model, module_model, brand_model, val_loader)\n    \n    scheduler_supergroup.step(avg_val_loss)\n    scheduler_group.step(avg_val_loss)\n    scheduler_module.step(avg_val_loss)\n    scheduler_brand.step(avg_val_loss)\n    print(\"*\"*15)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-10-27T10:06:02.361319Z","iopub.execute_input":"2024-10-27T10:06:02.361700Z","iopub.status.idle":"2024-10-27T10:06:46.612609Z","shell.execute_reply.started":"2024-10-27T10:06:02.361661Z","shell.execute_reply":"2024-10-27T10:06:46.611668Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74df4ca726ad479ab1c81422fbdc7d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038205356bed4839990dc05a08003bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/262k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"473b31c23a5c4d2b8c610ec0f77aaaf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"924c55e11f4a42d78c5b09fe74a0d449"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/2373772054.py:176: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  supergroup_model = torch.load('/kaggle/input/new-indoml-model/supergroup_model.pth').to(DEVICE)\n/tmp/ipykernel_30/2373772054.py:177: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  group_model = torch.load('/kaggle/input/new-indoml-model/group_model.pth').to(DEVICE)\n/tmp/ipykernel_30/2373772054.py:178: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  module_model = torch.load('/kaggle/input/new-indoml-model/module_model.pth').to(DEVICE)\n/tmp/ipykernel_30/2373772054.py:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  brand_model = torch.load('/kaggle/input/new-indoml-model/brand_model.pth').to(DEVICE)\n/tmp/ipykernel_30/2373772054.py:186: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n","output_type":"stream"}]},{"cell_type":"code","source":"import time\nfrom torch.profiler import profile, record_function, ProfilerActivity","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:07:01.352152Z","iopub.execute_input":"2024-10-27T10:07:01.352751Z","iopub.status.idle":"2024-10-27T10:07:01.357373Z","shell.execute_reply.started":"2024-10-27T10:07:01.352701Z","shell.execute_reply":"2024-10-27T10:07:01.356318Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"supergroup_model.train()\nsample_batch = next(iter(train_loader))  # Get one sample batch\nsample_batch = {k: v.to(DEVICE) for k, v in sample_batch.items()}\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n    input_ids = sample_batch['input_ids'].to(DEVICE)\n    attention_mask = sample_batch['attention_mask'].to(DEVICE)\n    labels1, labels2, labels3, labels4 = sample_batch['labels1'], sample_batch['labels2'], sample_batch['labels3'], sample_batch['labels4']\n\n       \n    with torch.amp.autocast('cuda'):\n        logits1 = supergroup_model(input_ids=input_ids, attention_mask=attention_mask)\n        loss1 = nn.CrossEntropyLoss()(logits1, labels1)\n    scaler.scale(loss1).backward()\n    scaler.step(optimizer_supergroup)\n    scaler.update()\n    optimizer_supergroup.zero_grad()\n\n    # Group model\n    with torch.amp.autocast('cuda'):\n        logits2 = group_model(input_ids=input_ids, attention_mask=attention_mask)\n        loss2 = nn.CrossEntropyLoss()(logits2, labels2)\n    scaler.scale(loss2).backward()\n    scaler.step(optimizer_group)\n    scaler.update()\n    optimizer_group.zero_grad()\n\n    with torch.amp.autocast('cuda'):\n        logits3 = module_model(input_ids=input_ids, attention_mask=attention_mask)\n        loss3 = nn.CrossEntropyLoss()(logits3, labels3)\n    scaler.scale(loss3).backward()\n    scaler.step(optimizer_module)\n    scaler.update()\n    optimizer_module.zero_grad()\n\n\n    with torch.amp.autocast('cuda'):\n        logits4 = brand_model(input_ids=input_ids, attention_mask=attention_mask)\n        loss4 = nn.CrossEntropyLoss()(logits4, labels4)\n    scaler.scale(loss4).backward()\n    scaler.step(optimizer_brand)\n    scaler.update()\n    optimizer_brand.zero_grad()\n\n\nprint(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\nprint(\"GFLOPs during training\") #GigaFLOPs\nprint(sum(k.flops for k in prof.key_averages())/1e9)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:15:31.811203Z","iopub.execute_input":"2024-10-27T10:15:31.811619Z","iopub.status.idle":"2024-10-27T10:15:43.080606Z","shell.execute_reply.started":"2024-10-27T10:15:31.811581Z","shell.execute_reply":"2024-10-27T10:15:43.079566Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                               aten::mm         0.93%       9.303ms         1.62%      16.119ms      54.457us      69.875ms        10.64%      69.875ms     236.063us           296    435493.405  \n                                            aten::addmm         3.52%      35.038ms         9.43%      93.855ms     617.467us      38.610ms         5.88%      38.610ms     254.011us           152    217897.697  \n                                              aten::mul         0.97%       9.622ms         2.93%      29.137ms      84.702us      11.600ms         1.77%      11.600ms      33.722us           344       598.914  \n                                              aten::add         0.49%       4.830ms         1.68%      16.743ms     321.986us     893.688us         0.14%     893.688us      17.186us            52        25.559  \n                                               aten::to         0.25%       2.498ms         3.27%      32.571ms      35.327us       0.000us         0.00%      12.545ms      13.606us           922            --  \n                                            aten::slice         0.52%       5.167ms         0.53%       5.293ms      57.530us       0.000us         0.00%       0.000us       0.000us            92            --  \n                                       aten::as_strided         0.32%       3.211ms         0.32%       3.211ms       1.204us       0.000us         0.00%       0.000us       0.000us          2668            --  \n                                           aten::expand         0.10%     965.525us         0.10%       1.002ms      31.315us       0.000us         0.00%       0.000us       0.000us            32            --  \n                                        aten::embedding         0.37%       3.645ms         9.19%      91.524ms       7.627ms       0.000us         0.00%     234.589us      19.549us            12            --  \n                                          aten::reshape         0.23%       2.283ms         1.53%      15.201ms      26.952us       0.000us         0.00%      13.696us       0.024us           564            --  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 995.420ms\nSelf CUDA time total: 656.425ms\n\nGFLOPs during training\n654.015575048\n","output_type":"stream"}]},{"cell_type":"code","source":"def predict(supergroup_model, group_model, module_model, brand_model, tokenizer, text):\n    start = time.time()\n    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n        with torch.no_grad():\n            input_ids = inputs['input_ids'].to(DEVICE)\n            attention_mask = inputs['attention_mask'].to(DEVICE)\n\n            # Supergroup prediction\n            logits1 = supergroup_model(input_ids=input_ids, attention_mask=attention_mask)\n            supergroup_preds = torch.argmax(torch.softmax(logits1, dim=1), dim=1)\n\n            # Group prediction\n            logits2 = group_model(input_ids=input_ids, attention_mask=attention_mask)\n            group_preds = group_model.predict(input_ids=input_ids, attention_mask=attention_mask, supergroup_preds=supergroup_preds)\n\n            # Module prediction\n            logits3 = module_model(input_ids=input_ids, attention_mask=attention_mask)\n            module_preds = module_model.predict(input_ids=input_ids, attention_mask=attention_mask, group_preds=group_preds)\n\n            # Brand prediction\n            logits4 = brand_model(input_ids=input_ids, attention_mask=attention_mask)\n            brand_preds = brand_model.predict(input_ids=input_ids, attention_mask=attention_mask, module_preds=module_preds)\n\n        supergroup = supergroup_encoder.inverse_transform([supergroup_preds.cpu().numpy()]),  # Detach from GPU, move to CPU, convert to NumPy, then inverse transform\n        group = group_encoder.inverse_transform([group_preds.cpu().numpy()]),       # Similar for group\n        module = module_encoder.inverse_transform([module_preds.cpu().numpy()]),      # Similar for module\n        brand = brand_encoder.inverse_transform([brand_preds.cpu().numpy()])    \n    print(\"Inference time :\"+str(time.time()-start))\n    #print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n    print(\"GFLOPs during testing\") #GigaFLOPs\n    print(sum(k.flops for k in prof.key_averages())/1e9)\n    return supergroup,group,module,brand\n# Example prediction\nsample_text = \"14 in hybrid blade\"\npredictions = predict(supergroup_model, group_model, module_model, brand_model, tokenizer, sample_text)\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T10:28:01.791047Z","iopub.execute_input":"2024-10-27T10:28:01.791441Z","iopub.status.idle":"2024-10-27T10:28:02.741195Z","shell.execute_reply.started":"2024-10-27T10:28:01.791408Z","shell.execute_reply":"2024-10-27T10:28:02.740252Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n","output_type":"stream"},{"name":"stdout","text":"Inference time :0.6291525363922119\nGFLOPs during testing\n11.92008192\n((array(['home do it yourself'], dtype=object),), (array(['home do it yourself detail unknown total'], dtype=object),), (array(['home do it yourself'], dtype=object),), array(['receipt all'], dtype=object))\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = pd.read_json('/kaggle/input/new-codalab-final-test/final_test_data/final_test_data.features',lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:56:25.936195Z","iopub.execute_input":"2024-10-20T08:56:25.936629Z","iopub.status.idle":"2024-10-20T08:56:26.643172Z","shell.execute_reply.started":"2024-10-20T08:56:25.936590Z","shell.execute_reply":"2024-10-20T08:56:26.642163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"description\"] = test_df[\"description\"] + \" \" +test_df[\"retailer\"].astype(str) + \" \"+ test_df[\"price\"].astype(str)\ntest_df[\"description\"] = test_df[\"description\"].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:56:38.952470Z","iopub.execute_input":"2024-10-20T08:56:38.953413Z","iopub.status.idle":"2024-10-20T08:56:39.251506Z","shell.execute_reply.started":"2024-10-20T08:56:38.953367Z","shell.execute_reply":"2024-10-20T08:56:39.250627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Tokenizing the input text\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            return_attention_mask=True,\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].squeeze(0)\n        attention_mask = encoding['attention_mask'].squeeze(0)\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n        }\n\n    \ndef create_test_loader(test_texts, tokenizer, max_length, batch_size):\n    test_dataset = TestDataset(\n        texts=test_texts,\n        tokenizer=tokenizer,\n        max_length=max_length\n    )\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    return test_loader\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"canwenxu/BERT-of-Theseus-MNLI\")","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:56:39.567200Z","iopub.execute_input":"2024-10-20T08:56:39.567862Z","iopub.status.idle":"2024-10-20T08:56:39.726088Z","shell.execute_reply.started":"2024-10-20T08:56:39.567819Z","shell.execute_reply":"2024-10-20T08:56:39.725097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loader = create_test_loader(test_df[\"description\"].tolist(),tokenizer,20,32)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:56:40.281827Z","iopub.execute_input":"2024-10-20T08:56:40.282491Z","iopub.status.idle":"2024-10-20T08:56:40.291291Z","shell.execute_reply.started":"2024-10-20T08:56:40.282448Z","shell.execute_reply":"2024-10-20T08:56:40.290312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(supergroup_model, group_model, module_model, brand_model, test_loader):\n    supergroup_model.eval()\n    group_model.eval()\n    module_model.eval()\n    brand_model.eval()\n\n    supergroup_preds_list = []\n    group_preds_list = []\n    module_preds_list = []\n    brand_preds_list = []\n\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Testing\"):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n\n            # 1. Predict supergroup\n            logits1 = supergroup_model(input_ids=input_ids, attention_mask=attention_mask)\n            supergroup_preds = torch.argmax(torch.softmax(logits1, dim=1), dim=1)\n\n            # 2. Predict group using predicted supergroup\n            group_preds = group_model.predict(input_ids=input_ids, attention_mask=attention_mask, supergroup_preds=supergroup_preds)\n\n            # 3. Predict module using predicted group\n            module_preds = module_model.predict(input_ids=input_ids, attention_mask=attention_mask, group_preds=group_preds)\n\n            # 4. Predict brand using predicted module\n            brand_preds = brand_model.predict(input_ids=input_ids, attention_mask=attention_mask, module_preds=module_preds)\n\n            # Store predictions for final evaluation or submission\n            supergroup_preds_list.extend(supergroup_preds.cpu().numpy())\n            group_preds_list.extend(group_preds.cpu().numpy())\n            module_preds_list.extend(module_preds.cpu().numpy())\n            brand_preds_list.extend(brand_preds.cpu().numpy())\n\n    # Return all predictions for further analysis or submission\n    return supergroup_preds_list, group_preds_list, module_preds_list, brand_preds_list\n\n\n# Making predictions with all models\nsupergroup_pred, group_pred, module_pred, brand_pred = test(supergroup_model, group_model, module_model, brand_model, test_loader)\n\nprint(\"Predictions are complete, now detaching and inverse transforming the predictions.\")\n\n# Detach, move to CPU, and inverse transform the predictions\ninverse_predictions = [\n    (\n        supergroup_encoder.inverse_transform([supergroup_pred[i]]),  # Detach from GPU, move to CPU, convert to NumPy, then inverse transform\n        group_encoder.inverse_transform([group_pred[i]]),       # Similar for group\n        module_encoder.inverse_transform([module_pred[i]]),      # Similar for module\n        brand_encoder.inverse_transform([brand_pred[i]])         # Similar for brand\n    )\n    for i in range(len(group_pred))  # Iterate through each set of predictions\n]\n\n# Extract predictions into separate lists\nsupergroup = []\ngroup = []\nmodule = []\nbrand = []\n\nfor (sg, g, md, br) in inverse_predictions:\n    supergroup.append(sg[0])\n    group.append(g[0])\n    module.append(md[0])\n    brand.append(br[0])\n\n# Adding the predictions to the test DataFrame\ntest_df['supergroup'] = supergroup\ntest_df['group'] = group\ntest_df['module'] = module\ntest_df['brand'] = brand","metadata":{"execution":{"iopub.status.busy":"2024-10-20T08:56:41.178124Z","iopub.execute_input":"2024-10-20T08:56:41.178873Z","iopub.status.idle":"2024-10-20T09:03:46.462811Z","shell.execute_reply.started":"2024-10-20T08:56:41.178836Z","shell.execute_reply":"2024-10-20T09:03:46.461997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2024-10-20T09:09:30.709233Z","iopub.execute_input":"2024-10-20T09:09:30.709699Z","iopub.status.idle":"2024-10-20T09:09:30.727024Z","shell.execute_reply.started":"2024-10-20T09:09:30.709658Z","shell.execute_reply":"2024-10-20T09:09:30.725999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('/kaggle/working/predictions.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T09:09:30.910219Z","iopub.execute_input":"2024-10-20T09:09:30.910583Z","iopub.status.idle":"2024-10-20T09:09:32.223977Z","shell.execute_reply.started":"2024-10-20T09:09:30.910547Z","shell.execute_reply":"2024-10-20T09:09:32.222917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(supergroup_model, 'supergroup_model.pth')\ntorch.save(group_model,'group_model.pth')\ntorch.save(module_model,'module_model.pth')\ntorch.save(brand_model,'brand_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-20T09:09:32.225695Z","iopub.execute_input":"2024-10-20T09:09:32.226009Z","iopub.status.idle":"2024-10-20T09:09:33.902253Z","shell.execute_reply.started":"2024-10-20T09:09:32.225977Z","shell.execute_reply":"2024-10-20T09:09:33.901445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}