{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523},{"sourceId":9722372,"sourceType":"datasetVersion","datasetId":5918644}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":38052.833242,"end_time":"2024-10-20T20:18:58.596341","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-20T09:44:45.763099","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"01925989089243cbb9e15007f97f3af2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef4cc65165da4bf9ae8ef70843112699","placeholder":"​","style":"IPY_MODEL_cd48d1ae40914b41ad361ac290005cb9","value":"config.json: 100%"}},"0ef5b99af2b94787b39565d71702577a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d21cdf962934eb1af13aa88c858bebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01925989089243cbb9e15007f97f3af2","IPY_MODEL_74223d432f3141ea8fd9f5a6b5694293","IPY_MODEL_3a61117d2bc047f19eb2db9f9fe07270"],"layout":"IPY_MODEL_d7028913e5874870b2325eef81ef9886"}},"23e9947680de4457a120969570567438":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a61117d2bc047f19eb2db9f9fe07270":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33d0e77600d4f358610a4e0c17c19d3","placeholder":"​","style":"IPY_MODEL_3c5bef47af8f41f38583fdb4efd2a2eb","value":" 474/474 [00:00&lt;00:00, 35.8kB/s]"}},"3c5bef47af8f41f38583fdb4efd2a2eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af851d6e5af44c2bcc97725f746ee5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d1efd5807f4ba09611b16c2151be36","placeholder":"​","style":"IPY_MODEL_cbb2918856284e8f899d9a4482aaa921","value":" 559M/559M [00:05&lt;00:00, 145MB/s]"}},"5a5007fac923412fa684d6eb62c286a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff10a6aed534990937f75f947401d2b","max":558614189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ef5b99af2b94787b39565d71702577a","value":558614189}},"5a99ffb87f2e445b99545f30ad6ee71e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f91cc07adf147d3802090697c48b7b6","placeholder":"​","style":"IPY_MODEL_b5ccdfea7b1a499a9ec9500779478afb","value":"pytorch_model.bin: 100%"}},"5ff10a6aed534990937f75f947401d2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d13f59deb1437e8e1d18867d6c03c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74223d432f3141ea8fd9f5a6b5694293":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab34e71fafa4d5084c1b80509818b3d","max":474,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23e9947680de4457a120969570567438","value":474}},"8f91cc07adf147d3802090697c48b7b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d1efd5807f4ba09611b16c2151be36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ccdfea7b1a499a9ec9500779478afb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33d0e77600d4f358610a4e0c17c19d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cab34e71fafa4d5084c1b80509818b3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb2918856284e8f899d9a4482aaa921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd48d1ae40914b41ad361ac290005cb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7028913e5874870b2325eef81ef9886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e78b9cc9dfc14ca6a6bbf0f3cb61d4ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a99ffb87f2e445b99545f30ad6ee71e","IPY_MODEL_5a5007fac923412fa684d6eb62c286a7","IPY_MODEL_4af851d6e5af44c2bcc97725f746ee5c"],"layout":"IPY_MODEL_69d13f59deb1437e8e1d18867d6c03c3"}},"ef4cc65165da4bf9ae8ef70843112699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.889203,"end_time":"2024-10-20T09:45:00.398902","exception":false,"start_time":"2024-10-20T09:44:48.509699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:42.799272Z","iopub.execute_input":"2024-10-25T18:01:42.800148Z","iopub.status.idle":"2024-10-25T18:01:52.942149Z","shell.execute_reply.started":"2024-10-25T18:01:42.800080Z","shell.execute_reply":"2024-10-25T18:01:52.941324Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"papermill":{"duration":0.172915,"end_time":"2024-10-20T09:45:00.589977","exception":false,"start_time":"2024-10-20T09:45:00.417062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:52.943800Z","iopub.execute_input":"2024-10-25T18:01:52.944099Z","iopub.status.idle":"2024-10-25T18:01:53.101625Z","shell.execute_reply.started":"2024-10-25T18:01:52.944066Z","shell.execute_reply":"2024-10-25T18:01:53.100829Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# df = df[:1000]","metadata":{"papermill":{"duration":0.022838,"end_time":"2024-10-20T09:45:00.629074","exception":false,"start_time":"2024-10-20T09:45:00.606236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:53.102672Z","iopub.execute_input":"2024-10-25T18:01:53.102962Z","iopub.status.idle":"2024-10-25T18:01:53.106968Z","shell.execute_reply.started":"2024-10-25T18:01:53.102932Z","shell.execute_reply":"2024-10-25T18:01:53.106139Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])\ndf['description'] = df['description'] + ' ' + df['retailer']","metadata":{"papermill":{"duration":0.864071,"end_time":"2024-10-20T09:45:01.509158","exception":false,"start_time":"2024-10-20T09:45:00.645087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:53.109009Z","iopub.execute_input":"2024-10-25T18:01:53.109298Z","iopub.status.idle":"2024-10-25T18:01:54.019012Z","shell.execute_reply.started":"2024-10-25T18:01:53.109267Z","shell.execute_reply":"2024-10-25T18:01:54.018220Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom torch.distributions import Categorical\nimport numpy as np","metadata":{"papermill":{"duration":2.23819,"end_time":"2024-10-20T09:45:03.763713","exception":false,"start_time":"2024-10-20T09:45:01.525523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:54.020114Z","iopub.execute_input":"2024-10-25T18:01:54.020417Z","iopub.status.idle":"2024-10-25T18:01:56.032729Z","shell.execute_reply.started":"2024-10-25T18:01:54.020384Z","shell.execute_reply":"2024-10-25T18:01:56.031650Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# For using the new one\n# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n\n# For loading the saved one\nmodel_dir = \"/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model\"\ntokenizer = DebertaTokenizer.from_pretrained(model_dir)","metadata":{"papermill":{"duration":0.127136,"end_time":"2024-10-20T09:45:03.907021","exception":false,"start_time":"2024-10-20T09:45:03.779885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:56.033839Z","iopub.execute_input":"2024-10-25T18:01:56.034270Z","iopub.status.idle":"2024-10-25T18:01:56.151958Z","shell.execute_reply.started":"2024-10-25T18:01:56.034238Z","shell.execute_reply":"2024-10-25T18:01:56.151174Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 12\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 32\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"papermill":{"duration":0.0918,"end_time":"2024-10-20T09:45:04.016272","exception":false,"start_time":"2024-10-20T09:45:03.924472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:56.153089Z","iopub.execute_input":"2024-10-25T18:01:56.153397Z","iopub.status.idle":"2024-10-25T18:01:56.187643Z","shell.execute_reply.started":"2024-10-25T18:01:56.153365Z","shell.execute_reply":"2024-10-25T18:01:56.186654Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"papermill":{"duration":0.031866,"end_time":"2024-10-20T09:45:04.066738","exception":false,"start_time":"2024-10-20T09:45:04.034872","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:56.188503Z","iopub.execute_input":"2024-10-25T18:01:56.188815Z","iopub.status.idle":"2024-10-25T18:01:56.198388Z","shell.execute_reply.started":"2024-10-25T18:01:56.188747Z","shell.execute_reply":"2024-10-25T18:01:56.197442Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"papermill":{"duration":0.032794,"end_time":"2024-10-20T09:45:04.120596","exception":false,"start_time":"2024-10-20T09:45:04.087802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:56.199428Z","iopub.execute_input":"2024-10-25T18:01:56.199753Z","iopub.status.idle":"2024-10-25T18:01:56.212679Z","shell.execute_reply.started":"2024-10-25T18:01:56.199721Z","shell.execute_reply":"2024-10-25T18:01:56.211926Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\n\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\n\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"papermill":{"duration":107.917166,"end_time":"2024-10-20T09:46:52.053567","exception":false,"start_time":"2024-10-20T09:45:04.136401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:01:56.215769Z","iopub.execute_input":"2024-10-25T18:01:56.216084Z","iopub.status.idle":"2024-10-25T18:03:13.932452Z","shell.execute_reply.started":"2024-10-25T18:01:56.216054Z","shell.execute_reply":"2024-10-25T18:03:13.931674Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.distributions import Categorical\nimport os\n\nclass AdvancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n        super().__init__()\n        # For loading the new model\n        self.deberta = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n        self.hidden_size = hidden_size\n        # Classifiers for each hierarchy level\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # RL Policy networks for each level\n        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # Contrastive learning projection head\n        self.projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim)\n        )\n        # Few-shot learning prototypes\n        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n        # Supervised classification logits\n        supergroup_logits = self.supergroup_classifier(hidden_states)\n        group_input = torch.cat([hidden_states, torch.softmax(supergroup_logits, dim=1)], dim=1)\n        group_logits = self.group_classifier(group_input)\n        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n        module_logits = self.module_classifier(module_input)\n        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n        brand_logits = self.brand_classifier(brand_input)\n        # RL policy logits\n        supergroup_policy = self.supergroup_policy(hidden_states)\n        group_policy = self.group_policy(group_input)\n        module_policy = self.module_policy(module_input)\n        brand_policy = self.brand_policy(brand_input)\n        # Contrastive learning projection\n        projection = self.projection(hidden_states)\n        # Few-shot learning\n        prototype_distances = torch.cdist(hidden_states, self.prototypes)\n        few_shot_logits = -prototype_distances  # Negative distance as logits\n        return (supergroup_logits, group_logits, module_logits, brand_logits), \\\n               (supergroup_policy, group_policy, module_policy, brand_policy), \\\n               projection, few_shot_logits\n    def sample_actions(self, policies):\n        return [Categorical(logits=policy).sample() for policy in policies]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:03:13.933619Z","iopub.execute_input":"2024-10-25T18:03:13.934006Z","iopub.status.idle":"2024-10-25T18:03:13.950012Z","shell.execute_reply.started":"2024-10-25T18:03:13.933964Z","shell.execute_reply":"2024-10-25T18:03:13.948935Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# class JointAccuracyTrainer:\n#     def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n#         self.model = model\n#         self.device = next(model.parameters()).device  # Get the device from the model's parameters\n#         self.supervised_optimizer = torch.optim.AdamW(model.parameters(), lr=supervised_lr)\n#         self.rl_optimizer = torch.optim.AdamW(model.parameters(), lr=rl_lr)\n#         self.criterion = nn.CrossEntropyLoss()\n#         self.contrastive_temperature = contrastive_temperature\n#         if loss_weights is None:\n#             self.loss_weights = [1.0, 1.0, 1.0, 1.0]\n#         else:\n#             self.loss_weights = loss_weights\n#     def compute_joint_loss(self, all_outputs, true_labels):\n#         # Unpack the model outputs\n#         supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n#         logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n#         # Compute classification losses for all levels\n#         loss_supergroup = self.criterion(logits_supergroup, true_labels['supergroup'])\n#         loss_group1 = self.criterion(logits_group1, true_labels['group1'])\n#         loss_group2 = self.criterion(logits_group2, true_labels['group2'])\n#         loss_group3 = self.criterion(logits_group3, true_labels['group3'])\n#         # Combine losses using weighted sum\n#         total_loss = (\n#             self.loss_weights[0] * loss_supergroup + \n#             self.loss_weights[1] * loss_group1 + \n#             self.loss_weights[2] * loss_group2 + \n#             self.loss_weights[3] * loss_group3\n#         )\n#         return total_loss\n        \n#     def supervised_step(self, batch, true_labels):\n#         self.supervised_optimizer.zero_grad()\n#         # Forward pass through the model\n#         all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n#         # Compute joint loss for all levels\n#         total_loss = self.compute_joint_loss(all_outputs, true_labels)\n#         # Backpropagate and update model weights\n#         total_loss.backward()\n#         torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n#         self.supervised_optimizer.step()\n#         return total_loss.item()\n        \n#     def validation_step(self, batch, true_labels):\n#         with torch.no_grad():\n#             # Forward pass during validation\n#             all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n#             supervised_logits, _, _, _ = all_outputs\n#             logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n#             # Compute predictions\n#             preds_supergroup = torch.argmax(logits_supergroup, dim=-1)\n#             preds_group1 = torch.argmax(logits_group1, dim=-1)\n#             preds_group2 = torch.argmax(logits_group2, dim=-1)\n#             preds_group3 = torch.argmax(logits_group3, dim=-1)\n#             # Compute accuracies\n#             supergroup_acc = (preds_supergroup == true_labels['supergroup']).float().mean().item()\n#             group1_acc = (preds_group1 == true_labels['group1']).float().mean().item()\n#             group2_acc = (preds_group2 == true_labels['group2']).float().mean().item()\n#             group3_acc = (preds_group3 == true_labels['group3']).float().mean().item()\n#             # Joint accuracy\n#             item_acc = ((preds_supergroup == true_labels['supergroup']) &\n#                        (preds_group1 == true_labels['group1']) &\n#                        (preds_group2 == true_labels['group2']) &\n#                        (preds_group3 == true_labels['group3'])).float().mean().item()\n#         return supergroup_acc, group1_acc, group2_acc, group3_acc, item_acc\n        \n# def train_and_evaluate(model, train_loader, val_loader, num_epochs=10):\n#     trainer = JointAccuracyTrainer(model)\n#     for epoch in range(num_epochs):\n#         model.train()\n#         total_sup_loss = 0.0\n#         # Training loop\n#         for batch in train_loader:\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             true_labels = {\n#                 'supergroup': batch['labels1'],\n#                 'group1': batch['labels2'],\n#                 'group2': batch['labels3'],\n#                 'group3': batch['labels4']\n#             }\n#             sup_loss = trainer.supervised_step(batch, true_labels)\n#             total_sup_loss += sup_loss\n#         # Validation loop\n#         model.eval()\n#         val_accuracies = [0, 0, 0, 0, 0]\n#         for batch in val_loader:\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             true_labels = {\n#                 'supergroup': batch['labels1'],\n#                 'group1': batch['labels2'],\n#                 'group2': batch['labels3'],\n#                 'group3': batch['labels4']\n#             }\n#             accs = trainer.validation_step(batch, true_labels)\n#             val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n#         # Average accuracies\n#         val_accuracies = [x / len(val_loader) for x in val_accuracies]\n#         print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n#               f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n#               f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, Group1: {val_accuracies[1]:.4f}, \"\n#               f\"Group2: {val_accuracies[2]:.4f}, Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}\")\n\n# # Usage\n# model = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\n# # For loading the model saved after pre-training the saved dict\n# model_path = '/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model/model.pth'\n# model.load_state_dict(torch.load(model_path))\n# # Assuming you have train_loader and val_loader already defined\n# train_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"papermill":{"duration":33669.587786,"end_time":"2024-10-20T19:08:01.658352","exception":false,"start_time":"2024-10-20T09:46:52.070566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-25T18:03:13.951283Z","iopub.execute_input":"2024-10-25T18:03:13.951569Z","iopub.status.idle":"2024-10-25T18:03:13.968490Z","shell.execute_reply.started":"2024-10-25T18:03:13.951538Z","shell.execute_reply":"2024-10-25T18:03:13.967570Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class JointAccuracyTrainer:\n    def __init__(self, model, supervised_lr=5e-6, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n        self.model = model\n        self.device = next(model.parameters()).device\n        \n        # Split parameters into pretrained and task-specific groups\n        pretrained_params = {'params': model.deberta.parameters(), 'lr': supervised_lr * 0.1}\n        task_params = {'params': [p for n, p in model.named_parameters() if not n.startswith('deberta')], \n                      'lr': supervised_lr}\n        \n        self.supervised_optimizer = torch.optim.AdamW(\n            [pretrained_params, task_params],\n            weight_decay=0.1,\n            betas=(0.9, 0.999)\n        )\n        \n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.supervised_optimizer,\n            T_0=5,\n            T_mult=1,\n            eta_min=supervised_lr * 0.1\n        )\n        \n        self.rl_optimizer = torch.optim.AdamW(model.parameters(), lr=rl_lr)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n        self.contrastive_temperature = contrastive_temperature\n        \n        if loss_weights is None:\n            self.loss_weights = [1.2, 1.0, 1.0, 1.0]\n        else:\n            self.loss_weights = loss_weights\n            \n        self.mixup_alpha = 0.4\n        \n    def mixup_batch(self, batch):\n        \"\"\"\n        Apply mixup to the entire batch while preserving attention masks\n        \"\"\"\n        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n        batch_size = batch['input_ids'].size(0)\n        index = torch.randperm(batch_size).to(self.device)\n        \n        mixed_batch = {}\n        \n        # Handle input_ids and attention_mask\n        mixed_batch['input_ids'] = batch['input_ids']  # Keep original input_ids\n        mixed_batch['attention_mask'] = batch['attention_mask']  # Keep original attention_mask\n        \n        # Handle labels with mixup\n        label_keys = ['labels1', 'labels2', 'labels3', 'labels4']\n        for key in label_keys:\n            if key in batch:\n                mixed_batch[key] = {\n                    'labels': batch[key],\n                    'mixed_labels': batch[key][index],\n                    'lambda': lam\n                }\n        \n        return mixed_batch, lam\n        \n    def compute_mixed_loss(self, logits, label_dict):\n        \"\"\"\n        Compute loss for mixed-up labels\n        \"\"\"\n        labels = label_dict['labels']\n        mixed_labels = label_dict['mixed_labels']\n        lam = label_dict['lambda']\n        \n        loss = lam * self.criterion(logits, labels) + (1 - lam) * self.criterion(logits, mixed_labels)\n        return loss\n\n    def compute_joint_loss(self, all_outputs, mixed_labels):\n        supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n        logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n        \n        # Compute losses with mixup\n        loss_supergroup = self.compute_mixed_loss(logits_supergroup, mixed_labels['labels1'])\n        loss_group1 = self.compute_mixed_loss(logits_group1, mixed_labels['labels2'])\n        loss_group2 = self.compute_mixed_loss(logits_group2, mixed_labels['labels3'])\n        loss_group3 = self.compute_mixed_loss(logits_group3, mixed_labels['labels4'])\n        \n        # Add L2 regularization\n        l2_reg = sum(torch.norm(param, 2) for param in self.model.parameters())\n        reg_loss = 0.01 * l2_reg\n        \n        total_loss = (\n            self.loss_weights[0] * loss_supergroup + \n            self.loss_weights[1] * loss_group1 + \n            self.loss_weights[2] * loss_group2 + \n            self.loss_weights[3] * loss_group3 + \n            reg_loss\n        )\n        return total_loss\n        \n    def supervised_step(self, batch):\n        self.supervised_optimizer.zero_grad()\n        \n        # Apply mixup to the batch\n        mixed_batch, _ = self.mixup_batch(batch)\n        \n        # Forward pass\n        all_outputs = self.model(\n            mixed_batch['input_ids'],\n            mixed_batch['attention_mask']\n        )\n        \n        # Compute loss\n        total_loss = self.compute_joint_loss(all_outputs, mixed_batch)\n        \n        # Backward pass\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n        self.supervised_optimizer.step()\n        \n        return total_loss.item()\n\n    def validation_step(self, batch):\n        \"\"\"\n        Perform a validation step without mixup\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.model(batch['input_ids'], batch['attention_mask'])\n            supervised_logits = outputs[0]\n            \n            # Compute accuracies for each level\n            accuracies = []\n            for i, logits in enumerate(supervised_logits):\n                preds = torch.argmax(logits, dim=1)\n                label_key = f'labels{i+1}'\n                acc = (preds == batch[label_key]).float().mean().item()\n                accuracies.append(acc)\n                \n            # Add combined accuracy\n            combined_correct = torch.all(\n                torch.stack([preds == batch[f'labels{i+1}'] for i, preds in enumerate(supervised_logits)]),\n                dim=0\n            )\n            accuracies.append(combined_correct.float().mean().item())\n            \n            return accuracies\n\ndef train_and_evaluate(model, train_loader, val_loader, num_epochs=30):\n    trainer = JointAccuracyTrainer(model)\n    best_accuracy = 0\n    best_epoch = 0\n    accumulation_steps = 4\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_sup_loss = 0.0\n        \n        for i, batch in enumerate(train_loader):\n            batch = {k: v.to(trainer.device) for k, v in batch.items()}\n            \n            sup_loss = trainer.supervised_step(batch) / accumulation_steps\n            total_sup_loss += sup_loss * accumulation_steps\n            \n            if (i + 1) % accumulation_steps == 0:\n                trainer.supervised_optimizer.step()\n                trainer.supervised_optimizer.zero_grad()\n        \n        # Validation phase\n        model.eval()\n        val_accuracies = [0, 0, 0, 0, 0]\n        for batch in val_loader:\n            batch = {k: v.to(trainer.device) for k, v in batch.items()}\n            accs = trainer.validation_step(batch)\n            val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n            \n        val_accuracies = [x / len(val_loader) for x in val_accuracies]\n        current_accuracy = val_accuracies[4]\n        \n        # Save best model\n        if current_accuracy > best_accuracy:\n            best_accuracy = current_accuracy\n            best_epoch = epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': trainer.supervised_optimizer.state_dict(),\n                'scheduler_state_dict': trainer.scheduler.state_dict(),\n                'accuracy': current_accuracy,\n                'best_accuracy': best_accuracy\n            }, 'best_model_checkpoint.pth')\n        \n        # Update learning rate\n        trainer.scheduler.step()\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n              f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, \"\n              f\"Group1: {val_accuracies[1]:.4f}, Group2: {val_accuracies[2]:.4f}, \"\n              f\"Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}, \"\n              f\"LR: {trainer.supervised_optimizer.param_groups[0]['lr']:.2e}\")\n        \n    print(f\"Best accuracy: {best_accuracy:.4f} achieved at epoch {best_epoch + 1}\")\n    return best_accuracy\n\n# Usage\nmodel = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, \n                                     num_modules=449, num_brands=5679).to(device)\n\n# Load previous checkpoint\nmodel_path = '/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model/model.pth'\nmodel.load_state_dict(torch.load(model_path))\n\n# Continue training\ntrain_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:03:13.969891Z","iopub.execute_input":"2024-10-25T18:03:13.970246Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf2da9b1bd644158f802eba68d82035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3e3aed3860344f6bd9ce8424a2bc3d7"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/3458504848.py:205: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n# Create the directory if it doesn't exist\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n# Save the model's state dictionary\n\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n# Save the tokenizer\n\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"papermill":{"duration":1.131605,"end_time":"2024-10-20T19:08:02.807942","exception":false,"start_time":"2024-10-20T19:08:01.676337","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"papermill":{"duration":0.872266,"end_time":"2024-10-20T19:08:03.698211","exception":false,"start_time":"2024-10-20T19:08:02.825945","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat['description'] = df_test_feat['description'] + ' ' + df_test_feat['retailer']\ndf_test_feat.head()","metadata":{"papermill":{"duration":0.037505,"end_time":"2024-10-20T19:08:03.753727","exception":false,"start_time":"2024-10-20T19:08:03.716222","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, text):\n    inputs = tokenizer(\n        text, \n        return_tensors=\"pt\", \n        truncation=True, \n        padding='max_length', \n        max_length=MAX_LENGTH\n    ).to(DEVICE)\n    with torch.no_grad():\n        logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n    predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n    return predictions\n\n# Example prediction\nsample_text = \"Short product description here\"\npredictions = predict(model, tokenizer, sample_text)\nprint(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")","metadata":{"papermill":{"duration":0.069928,"end_time":"2024-10-20T19:08:03.842254","exception":false,"start_time":"2024-10-20T19:08:03.772326","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_tmp = df_test_feat[:5]","metadata":{"papermill":{"duration":0.025659,"end_time":"2024-10-20T19:08:03.886458","exception":false,"start_time":"2024-10-20T19:08:03.860799","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(0, len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    with torch.no_grad():\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n            inputs = tokenizer(\n                df_test_feat.iloc[i].description, \n                return_tensors=\"pt\", \n                truncation=True, \n                padding='max_length', \n                max_length=MAX_LENGTH\n            ).to(DEVICE)\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n        try:\n            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n        except ValueError as e:\n            print(f\"Error in supergroups: {e}\")\n            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n        try:\n            groups_names = group_encoder.inverse_transform(groups_list)\n        except ValueError as e:\n            print(f\"Error in groups: {e}\")\n            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n        try:\n            modules_names = module_encoder.inverse_transform(modules_list)\n        except ValueError as e:\n            print(f\"Error in modules: {e}\")\n            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n        try:\n            brands_names = brand_encoder.inverse_transform(brands_list)\n        except ValueError as e:\n            print(f\"Error in brands: {e}\")\n            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n        # Create a DataFrame with predictions\n        predictions_df = pd.DataFrame({\n            'indoml_id': indoml_id_list,\n            'supergroup': supergroups_names,\n            'group': groups_names,\n            'module': modules_names,\n            'brand': brands_names\n        })\n        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n        print(\"predictions.predict saved\")\nprint(make_test_pred_and_save(df_test_feat))","metadata":{"papermill":{"duration":4252.569523,"end_time":"2024-10-20T20:18:56.474256","exception":false,"start_time":"2024-10-20T19:08:03.904733","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.031783,"end_time":"2024-10-20T20:18:56.538258","exception":false,"start_time":"2024-10-20T20:18:56.506475","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}