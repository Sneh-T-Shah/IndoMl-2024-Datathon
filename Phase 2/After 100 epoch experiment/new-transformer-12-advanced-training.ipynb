{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523},{"sourceId":9725163,"sourceType":"datasetVersion","datasetId":5918644}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":38052.833242,"end_time":"2024-10-20T20:18:58.596341","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-20T09:44:45.763099","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"01925989089243cbb9e15007f97f3af2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef4cc65165da4bf9ae8ef70843112699","placeholder":"​","style":"IPY_MODEL_cd48d1ae40914b41ad361ac290005cb9","value":"config.json: 100%"}},"0ef5b99af2b94787b39565d71702577a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d21cdf962934eb1af13aa88c858bebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01925989089243cbb9e15007f97f3af2","IPY_MODEL_74223d432f3141ea8fd9f5a6b5694293","IPY_MODEL_3a61117d2bc047f19eb2db9f9fe07270"],"layout":"IPY_MODEL_d7028913e5874870b2325eef81ef9886"}},"23e9947680de4457a120969570567438":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a61117d2bc047f19eb2db9f9fe07270":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33d0e77600d4f358610a4e0c17c19d3","placeholder":"​","style":"IPY_MODEL_3c5bef47af8f41f38583fdb4efd2a2eb","value":" 474/474 [00:00&lt;00:00, 35.8kB/s]"}},"3c5bef47af8f41f38583fdb4efd2a2eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af851d6e5af44c2bcc97725f746ee5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d1efd5807f4ba09611b16c2151be36","placeholder":"​","style":"IPY_MODEL_cbb2918856284e8f899d9a4482aaa921","value":" 559M/559M [00:05&lt;00:00, 145MB/s]"}},"5a5007fac923412fa684d6eb62c286a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff10a6aed534990937f75f947401d2b","max":558614189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ef5b99af2b94787b39565d71702577a","value":558614189}},"5a99ffb87f2e445b99545f30ad6ee71e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f91cc07adf147d3802090697c48b7b6","placeholder":"​","style":"IPY_MODEL_b5ccdfea7b1a499a9ec9500779478afb","value":"pytorch_model.bin: 100%"}},"5ff10a6aed534990937f75f947401d2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d13f59deb1437e8e1d18867d6c03c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74223d432f3141ea8fd9f5a6b5694293":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab34e71fafa4d5084c1b80509818b3d","max":474,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23e9947680de4457a120969570567438","value":474}},"8f91cc07adf147d3802090697c48b7b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d1efd5807f4ba09611b16c2151be36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ccdfea7b1a499a9ec9500779478afb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33d0e77600d4f358610a4e0c17c19d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cab34e71fafa4d5084c1b80509818b3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb2918856284e8f899d9a4482aaa921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd48d1ae40914b41ad361ac290005cb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7028913e5874870b2325eef81ef9886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e78b9cc9dfc14ca6a6bbf0f3cb61d4ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a99ffb87f2e445b99545f30ad6ee71e","IPY_MODEL_5a5007fac923412fa684d6eb62c286a7","IPY_MODEL_4af851d6e5af44c2bcc97725f746ee5c"],"layout":"IPY_MODEL_69d13f59deb1437e8e1d18867d6c03c3"}},"ef4cc65165da4bf9ae8ef70843112699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.889203,"end_time":"2024-10-20T09:45:00.398902","exception":false,"start_time":"2024-10-20T09:44:48.509699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:37:57.222360Z","iopub.execute_input":"2024-10-26T07:37:57.222663Z","iopub.status.idle":"2024-10-26T07:38:07.980486Z","shell.execute_reply.started":"2024-10-26T07:37:57.222613Z","shell.execute_reply":"2024-10-26T07:38:07.979653Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"papermill":{"duration":0.172915,"end_time":"2024-10-20T09:45:00.589977","exception":false,"start_time":"2024-10-20T09:45:00.417062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:07.982026Z","iopub.execute_input":"2024-10-26T07:38:07.982338Z","iopub.status.idle":"2024-10-26T07:38:08.131252Z","shell.execute_reply.started":"2024-10-26T07:38:07.982305Z","shell.execute_reply":"2024-10-26T07:38:08.130469Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# df = df[:10000]","metadata":{"papermill":{"duration":0.022838,"end_time":"2024-10-20T09:45:00.629074","exception":false,"start_time":"2024-10-20T09:45:00.606236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:08.132369Z","iopub.execute_input":"2024-10-26T07:38:08.132684Z","iopub.status.idle":"2024-10-26T07:38:08.136921Z","shell.execute_reply.started":"2024-10-26T07:38:08.132638Z","shell.execute_reply":"2024-10-26T07:38:08.135982Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])","metadata":{"papermill":{"duration":0.864071,"end_time":"2024-10-20T09:45:01.509158","exception":false,"start_time":"2024-10-20T09:45:00.645087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:08.139989Z","iopub.execute_input":"2024-10-26T07:38:08.140775Z","iopub.status.idle":"2024-10-26T07:38:08.913015Z","shell.execute_reply.started":"2024-10-26T07:38:08.140741Z","shell.execute_reply":"2024-10-26T07:38:08.912200Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom torch.distributions import Categorical\nimport numpy as np","metadata":{"papermill":{"duration":2.23819,"end_time":"2024-10-20T09:45:03.763713","exception":false,"start_time":"2024-10-20T09:45:01.525523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:08.914133Z","iopub.execute_input":"2024-10-26T07:38:08.914451Z","iopub.status.idle":"2024-10-26T07:38:10.886102Z","shell.execute_reply.started":"2024-10-26T07:38:08.914418Z","shell.execute_reply":"2024-10-26T07:38:10.885140Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# For using the new one\n# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n\n# For loading the saved one\nmodel_dir = \"/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model\"\ntokenizer = DebertaTokenizer.from_pretrained(model_dir)","metadata":{"papermill":{"duration":0.127136,"end_time":"2024-10-20T09:45:03.907021","exception":false,"start_time":"2024-10-20T09:45:03.779885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:10.887377Z","iopub.execute_input":"2024-10-26T07:38:10.887862Z","iopub.status.idle":"2024-10-26T07:38:11.000686Z","shell.execute_reply.started":"2024-10-26T07:38:10.887825Z","shell.execute_reply":"2024-10-26T07:38:10.999701Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 12\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 34\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"papermill":{"duration":0.0918,"end_time":"2024-10-20T09:45:04.016272","exception":false,"start_time":"2024-10-20T09:45:03.924472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:28.593129Z","iopub.execute_input":"2024-10-26T07:48:28.593511Z","iopub.status.idle":"2024-10-26T07:48:28.599084Z","shell.execute_reply.started":"2024-10-26T07:48:28.593473Z","shell.execute_reply":"2024-10-26T07:48:28.598198Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"papermill":{"duration":0.031866,"end_time":"2024-10-20T09:45:04.066738","exception":false,"start_time":"2024-10-20T09:45:04.034872","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:11.041362Z","iopub.execute_input":"2024-10-26T07:38:11.041782Z","iopub.status.idle":"2024-10-26T07:38:11.050299Z","shell.execute_reply.started":"2024-10-26T07:38:11.041749Z","shell.execute_reply":"2024-10-26T07:38:11.049456Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"papermill":{"duration":0.032794,"end_time":"2024-10-20T09:45:04.120596","exception":false,"start_time":"2024-10-20T09:45:04.087802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:11.051554Z","iopub.execute_input":"2024-10-26T07:38:11.051969Z","iopub.status.idle":"2024-10-26T07:38:11.063796Z","shell.execute_reply.started":"2024-10-26T07:38:11.051928Z","shell.execute_reply":"2024-10-26T07:38:11.063002Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\n\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\n\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"papermill":{"duration":107.917166,"end_time":"2024-10-20T09:46:52.053567","exception":false,"start_time":"2024-10-20T09:45:04.136401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:38:11.066803Z","iopub.execute_input":"2024-10-26T07:38:11.067141Z","iopub.status.idle":"2024-10-26T07:39:21.419557Z","shell.execute_reply.started":"2024-10-26T07:38:11.067112Z","shell.execute_reply":"2024-10-26T07:39:21.418546Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.distributions import Categorical\nimport os\n\nclass AdvancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n        super().__init__()\n        # For loading the new model\n        self.deberta = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n        self.hidden_size = hidden_size\n        # Classifiers for each hierarchy level\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # RL Policy networks for each level\n        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # Contrastive learning projection head\n        self.projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim)\n        )\n        # Few-shot learning prototypes\n        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n        # Supervised classification logits\n        supergroup_logits = self.supergroup_classifier(hidden_states)\n        group_input = torch.cat([hidden_states, torch.softmax(supergroup_logits, dim=1)], dim=1)\n        group_logits = self.group_classifier(group_input)\n        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n        module_logits = self.module_classifier(module_input)\n        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n        brand_logits = self.brand_classifier(brand_input)\n        # RL policy logits\n        supergroup_policy = self.supergroup_policy(hidden_states)\n        group_policy = self.group_policy(group_input)\n        module_policy = self.module_policy(module_input)\n        brand_policy = self.brand_policy(brand_input)\n        # Contrastive learning projection\n        projection = self.projection(hidden_states)\n        # Few-shot learning\n        prototype_distances = torch.cdist(hidden_states, self.prototypes)\n        few_shot_logits = -prototype_distances  # Negative distance as logits\n        return (supergroup_logits, group_logits, module_logits, brand_logits), \\\n               (supergroup_policy, group_policy, module_policy, brand_policy), \\\n               projection, few_shot_logits\n    def sample_actions(self, policies):\n        return [Categorical(logits=policy).sample() for policy in policies]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T07:39:21.420833Z","iopub.execute_input":"2024-10-26T07:39:21.421115Z","iopub.status.idle":"2024-10-26T07:39:21.436754Z","shell.execute_reply.started":"2024-10-26T07:39:21.421085Z","shell.execute_reply":"2024-10-26T07:39:21.435703Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# class JointAccuracyTrainer:\n#     def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n#         self.model = model\n#         self.device = next(model.parameters()).device  # Get the device from the model's parameters\n#         self.supervised_optimizer = torch.optim.AdamW(model.parameters(), lr=supervised_lr)\n#         self.rl_optimizer = torch.optim.AdamW(model.parameters(), lr=rl_lr)\n#         self.criterion = nn.CrossEntropyLoss()\n#         self.contrastive_temperature = contrastive_temperature\n#         if loss_weights is None:\n#             self.loss_weights = [1.0, 1.0, 1.0, 1.0]\n#         else:\n#             self.loss_weights = loss_weights\n#     def compute_joint_loss(self, all_outputs, true_labels):\n#         # Unpack the model outputs\n#         supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n#         logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n#         # Compute classification losses for all levels\n#         loss_supergroup = self.criterion(logits_supergroup, true_labels['supergroup'])\n#         loss_group1 = self.criterion(logits_group1, true_labels['group1'])\n#         loss_group2 = self.criterion(logits_group2, true_labels['group2'])\n#         loss_group3 = self.criterion(logits_group3, true_labels['group3'])\n#         # Combine losses using weighted sum\n#         total_loss = (\n#             self.loss_weights[0] * loss_supergroup + \n#             self.loss_weights[1] * loss_group1 + \n#             self.loss_weights[2] * loss_group2 + \n#             self.loss_weights[3] * loss_group3\n#         )\n#         return total_loss\n        \n#     def supervised_step(self, batch, true_labels):\n#         self.supervised_optimizer.zero_grad()\n#         # Forward pass through the model\n#         all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n#         # Compute joint loss for all levels\n#         total_loss = self.compute_joint_loss(all_outputs, true_labels)\n#         # Backpropagate and update model weights\n#         total_loss.backward()\n#         torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n#         self.supervised_optimizer.step()\n#         return total_loss.item()\n        \n#     def validation_step(self, batch, true_labels):\n#         with torch.no_grad():\n#             # Forward pass during validation\n#             all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n#             supervised_logits, _, _, _ = all_outputs\n#             logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n#             # Compute predictions\n#             preds_supergroup = torch.argmax(logits_supergroup, dim=-1)\n#             preds_group1 = torch.argmax(logits_group1, dim=-1)\n#             preds_group2 = torch.argmax(logits_group2, dim=-1)\n#             preds_group3 = torch.argmax(logits_group3, dim=-1)\n#             # Compute accuracies\n#             supergroup_acc = (preds_supergroup == true_labels['supergroup']).float().mean().item()\n#             group1_acc = (preds_group1 == true_labels['group1']).float().mean().item()\n#             group2_acc = (preds_group2 == true_labels['group2']).float().mean().item()\n#             group3_acc = (preds_group3 == true_labels['group3']).float().mean().item()\n#             # Joint accuracy\n#             item_acc = ((preds_supergroup == true_labels['supergroup']) &\n#                        (preds_group1 == true_labels['group1']) &\n#                        (preds_group2 == true_labels['group2']) &\n#                        (preds_group3 == true_labels['group3'])).float().mean().item()\n#         return supergroup_acc, group1_acc, group2_acc, group3_acc, item_acc\n        \n# def train_and_evaluate(model, train_loader, val_loader, num_epochs=10):\n#     trainer = JointAccuracyTrainer(model)\n#     for epoch in range(num_epochs):\n#         model.train()\n#         total_sup_loss = 0.0\n#         # Training loop\n#         for batch in train_loader:\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             true_labels = {\n#                 'supergroup': batch['labels1'],\n#                 'group1': batch['labels2'],\n#                 'group2': batch['labels3'],\n#                 'group3': batch['labels4']\n#             }\n#             sup_loss = trainer.supervised_step(batch, true_labels)\n#             total_sup_loss += sup_loss\n#         # Validation loop\n#         model.eval()\n#         val_accuracies = [0, 0, 0, 0, 0]\n#         for batch in val_loader:\n#             batch = {k: v.to(device) for k, v in batch.items()}\n#             true_labels = {\n#                 'supergroup': batch['labels1'],\n#                 'group1': batch['labels2'],\n#                 'group2': batch['labels3'],\n#                 'group3': batch['labels4']\n#             }\n#             accs = trainer.validation_step(batch, true_labels)\n#             val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n#         # Average accuracies\n#         val_accuracies = [x / len(val_loader) for x in val_accuracies]\n#         print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n#               f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n#               f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, Group1: {val_accuracies[1]:.4f}, \"\n#               f\"Group2: {val_accuracies[2]:.4f}, Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}\")\n\n# # Usage\n# model = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\n# # For loading the model saved after pre-training the saved dict\n# model_path = '/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model/model.pth'\n# model.load_state_dict(torch.load(model_path))\n# # Assuming you have train_loader and val_loader already defined\n# train_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"papermill":{"duration":33669.587786,"end_time":"2024-10-20T19:08:01.658352","exception":false,"start_time":"2024-10-20T09:46:52.070566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:39:21.438163Z","iopub.execute_input":"2024-10-26T07:39:21.438455Z","iopub.status.idle":"2024-10-26T07:39:21.455848Z","shell.execute_reply.started":"2024-10-26T07:39:21.438424Z","shell.execute_reply":"2024-10-26T07:39:21.455079Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class JointAccuracyTrainer:\n    def __init__(self, model, supervised_lr=5e-6, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n        self.model = model\n        self.device = next(model.parameters()).device\n        \n        # Split parameters into pretrained and task-specific groups\n        pretrained_params = {'params': model.deberta.parameters(), 'lr': supervised_lr * 0.1}\n        task_params = {'params': [p for n, p in model.named_parameters() if not n.startswith('deberta')], \n                      'lr': supervised_lr}\n        \n        self.supervised_optimizer = torch.optim.AdamW(\n            [pretrained_params, task_params],\n            weight_decay=0.1,\n            betas=(0.9, 0.999)\n        )\n        \n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n            self.supervised_optimizer,\n            T_0=5,\n            T_mult=1,\n            eta_min=supervised_lr * 0.1\n        )\n        \n        self.rl_optimizer = torch.optim.AdamW(model.parameters(), lr=rl_lr)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.15)\n        self.contrastive_temperature = contrastive_temperature\n        \n        if loss_weights is None:\n            self.loss_weights = [1.2, 1.0, 1.0, 1.0]\n        else:\n            self.loss_weights = loss_weights\n            \n        self.mixup_alpha = 0.2  # Reduced from 0.4 to make mixing less aggressive\n        \n    def mixup_data(self, input_ids, attention_mask, labels_dict):\n        \"\"\"\n        Applies mixup to input data and labels\n        \"\"\"\n        batch_size = input_ids.size(0)\n        \n        # Generate mixup parameter and permutation\n        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n        lam = max(lam, 1-lam)  # Ensure lambda is at least 0.5 to preserve majority class\n        index = torch.randperm(batch_size).to(self.device)\n        \n        # Mix the embeddings rather than raw input_ids\n        mixed_input_ids = input_ids  # Keep original input_ids\n        mixed_attention_mask = attention_mask  # Keep original attention_mask\n        \n        # Mix the labels\n        mixed_labels = {}\n        for key, labels in labels_dict.items():\n            mixed_labels[key] = labels  # Keep original labels for computing both losses\n            mixed_labels[f\"{key}_mixed\"] = labels[index]  # Store permuted labels\n            mixed_labels[f\"{key}_lambda\"] = lam\n            \n        return mixed_input_ids, mixed_attention_mask, mixed_labels\n        \n    def compute_loss(self, logits, labels, mixed_labels, lam):\n        \"\"\"\n        Compute loss with mixup\n        \"\"\"\n        return lam * self.criterion(logits, labels) + (1 - lam) * self.criterion(logits, mixed_labels)\n\n    def compute_joint_loss(self, all_outputs, labels_dict):\n        supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n        logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n        \n        # Compute losses with proper label handling\n        losses = []\n        for i, logits in enumerate(supervised_logits):\n            label_key = f'labels{i+1}'\n            labels = labels_dict[label_key]\n            mixed_labels = labels_dict[f\"{label_key}_mixed\"]\n            lam = labels_dict[f\"{label_key}_lambda\"]\n            \n            loss = self.compute_loss(logits, labels, mixed_labels, lam)\n            losses.append(loss * self.loss_weights[i])\n        \n        # Add L2 regularization\n        l2_reg = 0\n        for param in self.model.parameters():\n            l2_reg += torch.norm(param, 2)\n        reg_loss = 0.01 * l2_reg\n        \n        total_loss = sum(losses) + reg_loss\n        return total_loss\n        \n    def supervised_step(self, batch):\n        self.supervised_optimizer.zero_grad()\n        \n        # Prepare labels dictionary\n        labels_dict = {\n            'labels1': batch['labels1'],\n            'labels2': batch['labels2'],\n            'labels3': batch['labels3'],\n            'labels4': batch['labels4']\n        }\n        \n        # Apply mixup\n        mixed_ids, mixed_mask, mixed_labels = self.mixup_data(\n            batch['input_ids'],\n            batch['attention_mask'],\n            labels_dict\n        )\n        \n        # Forward pass\n        all_outputs = self.model(mixed_ids, mixed_mask)\n        \n        # Compute loss\n        total_loss = self.compute_joint_loss(all_outputs, mixed_labels)\n        \n        # Backward pass\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n        self.supervised_optimizer.step()\n        \n        return total_loss.item()\n\n    def validation_step(self, batch):\n        \"\"\"\n        Perform a validation step without mixup\n        \"\"\"\n        with torch.no_grad():\n            outputs = self.model(batch['input_ids'], batch['attention_mask'])\n            supervised_logits = outputs[0]\n            \n            accuracies = []\n            correct_predictions = None\n            \n            # Compute accuracies for each level\n            for i, logits in enumerate(supervised_logits):\n                preds = torch.argmax(logits, dim=1)\n                label_key = f'labels{i+1}'\n                current_correct = (preds == batch[label_key])\n                acc = current_correct.float().mean().item()\n                accuracies.append(acc)\n                \n                if correct_predictions is None:\n                    correct_predictions = current_correct\n                else:\n                    correct_predictions &= current_correct\n            \n            # Compute all-levels accuracy\n            overall_acc = correct_predictions.float().mean().item()\n            accuracies.append(overall_acc)\n            \n            return accuracies\n\ndef train_and_evaluate(model, train_loader, val_loader, num_epochs=30):\n    trainer = JointAccuracyTrainer(model)\n    best_accuracy = 0\n    best_epoch = 0\n    accumulation_steps = 4\n    \n    for epoch in range(num_epochs):\n        model.train()\n        total_sup_loss = 0.0\n        \n        for i, batch in enumerate(train_loader):\n            batch = {k: v.to(trainer.device) for k, v in batch.items()}\n            \n            # Compute loss with gradient accumulation\n            sup_loss = trainer.supervised_step(batch) / accumulation_steps\n            total_sup_loss += sup_loss * accumulation_steps\n            \n            if (i + 1) % accumulation_steps == 0:\n                trainer.supervised_optimizer.step()\n                trainer.supervised_optimizer.zero_grad()\n        \n        # Validation phase\n        model.eval()\n        val_accuracies = [0, 0, 0, 0, 0]\n        num_batches = 0\n        \n        for batch in val_loader:\n            batch = {k: v.to(trainer.device) for k, v in batch.items()}\n            accs = trainer.validation_step(batch)\n            val_accuracies = [a + b for a, b in zip(val_accuracies, accs)]\n            num_batches += 1\n            \n        val_accuracies = [acc / num_batches for acc in val_accuracies]\n        current_accuracy = val_accuracies[4]  # Overall accuracy\n        \n        # Save best model\n        if current_accuracy > best_accuracy:\n            best_accuracy = current_accuracy\n            best_epoch = epoch\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': trainer.supervised_optimizer.state_dict(),\n                'scheduler_state_dict': trainer.scheduler.state_dict(),\n                'accuracy': current_accuracy,\n                'best_accuracy': best_accuracy\n            }, 'best_model_checkpoint.pth')\n        \n        # Update learning rate\n        trainer.scheduler.step()\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n              f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, \"\n              f\"Group1: {val_accuracies[1]:.4f}, Group2: {val_accuracies[2]:.4f}, \"\n              f\"Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}, \"\n              f\"LR: {trainer.supervised_optimizer.param_groups[0]['lr']:.2e}\")\n        \n    print(f\"Best accuracy: {best_accuracy:.4f} achieved at epoch {best_epoch + 1}\")\n    return best_accuracy\n\n# Usage\nmodel = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, \n                                     num_modules=449, num_brands=5679).to(device)\n\n# Load previous checkpoint\nmodel_path = '/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model/model.pth'\nmodel.load_state_dict(torch.load(model_path))\n\n# Continue training\ntrain_and_evaluate(model, train_loader, val_loader, num_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T07:48:32.901797Z","iopub.execute_input":"2024-10-26T07:48:32.902708Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/426035375.py:217: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/34 - Train Loss: 78.5958, Val Accuracies - Supergroup: 0.8827, Group1: 0.8561, Group2: 0.8443, Group3: 0.8622, Item Accuracy: 0.7632, LR: 5.00e-07\nEpoch 2/34 - Train Loss: 75.5069, Val Accuracies - Supergroup: 0.8846, Group1: 0.8585, Group2: 0.8473, Group3: 0.8669, Item Accuracy: 0.7708, LR: 5.00e-07\nEpoch 3/34 - Train Loss: 73.6866, Val Accuracies - Supergroup: 0.8856, Group1: 0.8597, Group2: 0.8487, Group3: 0.8686, Item Accuracy: 0.7740, LR: 5.00e-07\nEpoch 4/34 - Train Loss: 72.3978, Val Accuracies - Supergroup: 0.8860, Group1: 0.8603, Group2: 0.8490, Group3: 0.8690, Item Accuracy: 0.7751, LR: 5.00e-07\nEpoch 5/34 - Train Loss: 71.3649, Val Accuracies - Supergroup: 0.8864, Group1: 0.8607, Group2: 0.8494, Group3: 0.8698, Item Accuracy: 0.7769, LR: 5.00e-07\nEpoch 6/34 - Train Loss: 70.0033, Val Accuracies - Supergroup: 0.8868, Group1: 0.8612, Group2: 0.8499, Group3: 0.8712, Item Accuracy: 0.7783, LR: 5.00e-07\nEpoch 7/34 - Train Loss: 68.2473, Val Accuracies - Supergroup: 0.8870, Group1: 0.8607, Group2: 0.8498, Group3: 0.8705, Item Accuracy: 0.7779, LR: 5.00e-07\nEpoch 8/34 - Train Loss: 66.7944, Val Accuracies - Supergroup: 0.8874, Group1: 0.8614, Group2: 0.8500, Group3: 0.8732, Item Accuracy: 0.7811, LR: 5.00e-07\nEpoch 9/34 - Train Loss: 65.6370, Val Accuracies - Supergroup: 0.8872, Group1: 0.8615, Group2: 0.8504, Group3: 0.8724, Item Accuracy: 0.7806, LR: 5.00e-07\nEpoch 10/34 - Train Loss: 64.9311, Val Accuracies - Supergroup: 0.8874, Group1: 0.8615, Group2: 0.8501, Group3: 0.8719, Item Accuracy: 0.7803, LR: 5.00e-07\nEpoch 11/34 - Train Loss: 63.8269, Val Accuracies - Supergroup: 0.8869, Group1: 0.8611, Group2: 0.8497, Group3: 0.8714, Item Accuracy: 0.7795, LR: 5.00e-07\nEpoch 12/34 - Train Loss: 62.3419, Val Accuracies - Supergroup: 0.8875, Group1: 0.8616, Group2: 0.8499, Group3: 0.8728, Item Accuracy: 0.7813, LR: 5.00e-07\nEpoch 13/34 - Train Loss: 61.1481, Val Accuracies - Supergroup: 0.8874, Group1: 0.8615, Group2: 0.8497, Group3: 0.8720, Item Accuracy: 0.7803, LR: 5.00e-07\nEpoch 14/34 - Train Loss: 60.1486, Val Accuracies - Supergroup: 0.8874, Group1: 0.8618, Group2: 0.8499, Group3: 0.8730, Item Accuracy: 0.7816, LR: 5.00e-07\nEpoch 15/34 - Train Loss: 59.5915, Val Accuracies - Supergroup: 0.8874, Group1: 0.8616, Group2: 0.8498, Group3: 0.8733, Item Accuracy: 0.7818, LR: 5.00e-07\nEpoch 16/34 - Train Loss: 58.6265, Val Accuracies - Supergroup: 0.8872, Group1: 0.8612, Group2: 0.8494, Group3: 0.8739, Item Accuracy: 0.7821, LR: 5.00e-07\nEpoch 17/34 - Train Loss: 57.4042, Val Accuracies - Supergroup: 0.8873, Group1: 0.8612, Group2: 0.8496, Group3: 0.8736, Item Accuracy: 0.7822, LR: 5.00e-07\nEpoch 18/34 - Train Loss: 56.3886, Val Accuracies - Supergroup: 0.8872, Group1: 0.8608, Group2: 0.8494, Group3: 0.8732, Item Accuracy: 0.7816, LR: 5.00e-07\nEpoch 19/34 - Train Loss: 55.6598, Val Accuracies - Supergroup: 0.8869, Group1: 0.8605, Group2: 0.8492, Group3: 0.8724, Item Accuracy: 0.7807, LR: 5.00e-07\nEpoch 20/34 - Train Loss: 55.1627, Val Accuracies - Supergroup: 0.8870, Group1: 0.8608, Group2: 0.8492, Group3: 0.8728, Item Accuracy: 0.7813, LR: 5.00e-07\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n# Create the directory if it doesn't exist\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n# Save the model's state dictionary\n\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n# Save the tokenizer\n\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"papermill":{"duration":1.131605,"end_time":"2024-10-20T19:08:02.807942","exception":false,"start_time":"2024-10-20T19:08:01.676337","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.596770Z","iopub.status.idle":"2024-10-26T07:48:17.597121Z","shell.execute_reply.started":"2024-10-26T07:48:17.596950Z","shell.execute_reply":"2024-10-26T07:48:17.596967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"papermill":{"duration":0.872266,"end_time":"2024-10-20T19:08:03.698211","exception":false,"start_time":"2024-10-20T19:08:02.825945","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.598614Z","iopub.status.idle":"2024-10-26T07:48:17.599010Z","shell.execute_reply.started":"2024-10-26T07:48:17.598837Z","shell.execute_reply":"2024-10-26T07:48:17.598856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat.head()","metadata":{"papermill":{"duration":0.037505,"end_time":"2024-10-20T19:08:03.753727","exception":false,"start_time":"2024-10-20T19:08:03.716222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.600203Z","iopub.status.idle":"2024-10-26T07:48:17.600611Z","shell.execute_reply.started":"2024-10-26T07:48:17.600421Z","shell.execute_reply":"2024-10-26T07:48:17.600441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, text):\n    inputs = tokenizer(\n        text, \n        return_tensors=\"pt\", \n        truncation=True, \n        padding='max_length', \n        max_length=MAX_LENGTH\n    ).to(DEVICE)\n    with torch.no_grad():\n        logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n    predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n    return predictions\n\n# Example prediction\nsample_text = \"Short product description here\"\npredictions = predict(model, tokenizer, sample_text)\nprint(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")","metadata":{"papermill":{"duration":0.069928,"end_time":"2024-10-20T19:08:03.842254","exception":false,"start_time":"2024-10-20T19:08:03.772326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.601855Z","iopub.status.idle":"2024-10-26T07:48:17.602184Z","shell.execute_reply.started":"2024-10-26T07:48:17.602018Z","shell.execute_reply":"2024-10-26T07:48:17.602035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_tmp = df_test_feat[:5]","metadata":{"papermill":{"duration":0.025659,"end_time":"2024-10-20T19:08:03.886458","exception":false,"start_time":"2024-10-20T19:08:03.860799","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.603261Z","iopub.status.idle":"2024-10-26T07:48:17.603616Z","shell.execute_reply.started":"2024-10-26T07:48:17.603440Z","shell.execute_reply":"2024-10-26T07:48:17.603458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(0, len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    with torch.no_grad():\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n            inputs = tokenizer(\n                df_test_feat.iloc[i].description, \n                return_tensors=\"pt\", \n                truncation=True, \n                padding='max_length', \n                max_length=MAX_LENGTH\n            ).to(DEVICE)\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n        try:\n            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n        except ValueError as e:\n            print(f\"Error in supergroups: {e}\")\n            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n        try:\n            groups_names = group_encoder.inverse_transform(groups_list)\n        except ValueError as e:\n            print(f\"Error in groups: {e}\")\n            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n        try:\n            modules_names = module_encoder.inverse_transform(modules_list)\n        except ValueError as e:\n            print(f\"Error in modules: {e}\")\n            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n        try:\n            brands_names = brand_encoder.inverse_transform(brands_list)\n        except ValueError as e:\n            print(f\"Error in brands: {e}\")\n            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n        # Create a DataFrame with predictions\n        predictions_df = pd.DataFrame({\n            'indoml_id': indoml_id_list,\n            'supergroup': supergroups_names,\n            'group': groups_names,\n            'module': modules_names,\n            'brand': brands_names\n        })\n        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n        print(\"predictions.predict saved\")\nprint(make_test_pred_and_save(df_test_feat))","metadata":{"papermill":{"duration":4252.569523,"end_time":"2024-10-20T20:18:56.474256","exception":false,"start_time":"2024-10-20T19:08:03.904733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-26T07:48:17.605556Z","iopub.status.idle":"2024-10-26T07:48:17.605946Z","shell.execute_reply.started":"2024-10-26T07:48:17.605763Z","shell.execute_reply":"2024-10-26T07:48:17.605782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.031783,"end_time":"2024-10-20T20:18:56.538258","exception":false,"start_time":"2024-10-20T20:18:56.506475","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}