{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523},{"sourceId":9733080,"sourceType":"datasetVersion","datasetId":5918644}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":38052.833242,"end_time":"2024-10-20T20:18:58.596341","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-20T09:44:45.763099","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"01925989089243cbb9e15007f97f3af2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef4cc65165da4bf9ae8ef70843112699","placeholder":"​","style":"IPY_MODEL_cd48d1ae40914b41ad361ac290005cb9","value":"config.json: 100%"}},"0ef5b99af2b94787b39565d71702577a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d21cdf962934eb1af13aa88c858bebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01925989089243cbb9e15007f97f3af2","IPY_MODEL_74223d432f3141ea8fd9f5a6b5694293","IPY_MODEL_3a61117d2bc047f19eb2db9f9fe07270"],"layout":"IPY_MODEL_d7028913e5874870b2325eef81ef9886"}},"23e9947680de4457a120969570567438":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a61117d2bc047f19eb2db9f9fe07270":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33d0e77600d4f358610a4e0c17c19d3","placeholder":"​","style":"IPY_MODEL_3c5bef47af8f41f38583fdb4efd2a2eb","value":" 474/474 [00:00&lt;00:00, 35.8kB/s]"}},"3c5bef47af8f41f38583fdb4efd2a2eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4af851d6e5af44c2bcc97725f746ee5c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91d1efd5807f4ba09611b16c2151be36","placeholder":"​","style":"IPY_MODEL_cbb2918856284e8f899d9a4482aaa921","value":" 559M/559M [00:05&lt;00:00, 145MB/s]"}},"5a5007fac923412fa684d6eb62c286a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ff10a6aed534990937f75f947401d2b","max":558614189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0ef5b99af2b94787b39565d71702577a","value":558614189}},"5a99ffb87f2e445b99545f30ad6ee71e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f91cc07adf147d3802090697c48b7b6","placeholder":"​","style":"IPY_MODEL_b5ccdfea7b1a499a9ec9500779478afb","value":"pytorch_model.bin: 100%"}},"5ff10a6aed534990937f75f947401d2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69d13f59deb1437e8e1d18867d6c03c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74223d432f3141ea8fd9f5a6b5694293":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cab34e71fafa4d5084c1b80509818b3d","max":474,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23e9947680de4457a120969570567438","value":474}},"8f91cc07adf147d3802090697c48b7b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d1efd5807f4ba09611b16c2151be36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5ccdfea7b1a499a9ec9500779478afb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c33d0e77600d4f358610a4e0c17c19d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cab34e71fafa4d5084c1b80509818b3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb2918856284e8f899d9a4482aaa921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd48d1ae40914b41ad361ac290005cb9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7028913e5874870b2325eef81ef9886":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e78b9cc9dfc14ca6a6bbf0f3cb61d4ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a99ffb87f2e445b99545f30ad6ee71e","IPY_MODEL_5a5007fac923412fa684d6eb62c286a7","IPY_MODEL_4af851d6e5af44c2bcc97725f746ee5c"],"layout":"IPY_MODEL_69d13f59deb1437e8e1d18867d6c03c3"}},"ef4cc65165da4bf9ae8ef70843112699":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Do not use the data saved in v4 version of data as it is from v6 added the retailer thing</h1>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.889203,"end_time":"2024-10-20T09:45:00.398902","exception":false,"start_time":"2024-10-20T09:44:48.509699","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:01.840156Z","iopub.execute_input":"2024-10-27T09:04:01.840435Z","iopub.status.idle":"2024-10-27T09:04:12.459832Z","shell.execute_reply.started":"2024-10-27T09:04:01.840404Z","shell.execute_reply":"2024-10-27T09:04:12.458961Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")","metadata":{"papermill":{"duration":0.172915,"end_time":"2024-10-20T09:45:00.589977","exception":false,"start_time":"2024-10-20T09:45:00.417062","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.461312Z","iopub.execute_input":"2024-10-27T09:04:12.461618Z","iopub.status.idle":"2024-10-27T09:04:12.609590Z","shell.execute_reply.started":"2024-10-27T09:04:12.461585Z","shell.execute_reply":"2024-10-27T09:04:12.608795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# df = df[:10]","metadata":{"papermill":{"duration":0.022838,"end_time":"2024-10-20T09:45:00.629074","exception":false,"start_time":"2024-10-20T09:45:00.606236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.610805Z","iopub.execute_input":"2024-10-27T09:04:12.611104Z","iopub.status.idle":"2024-10-27T09:04:12.615820Z","shell.execute_reply.started":"2024-10-27T09:04:12.611072Z","shell.execute_reply":"2024-10-27T09:04:12.614598Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])","metadata":{"papermill":{"duration":0.864071,"end_time":"2024-10-20T09:45:01.509158","exception":false,"start_time":"2024-10-20T09:45:00.645087","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.618378Z","iopub.execute_input":"2024-10-27T09:04:12.618745Z","iopub.status.idle":"2024-10-27T09:04:12.657144Z","shell.execute_reply.started":"2024-10-27T09:04:12.618704Z","shell.execute_reply":"2024-10-27T09:04:12.656197Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom torch.distributions import Categorical\nimport numpy as np\nimport time\nfrom torch.profiler import profile, record_function, ProfilerActivity","metadata":{"papermill":{"duration":2.23819,"end_time":"2024-10-20T09:45:03.763713","exception":false,"start_time":"2024-10-20T09:45:01.525523","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.658261Z","iopub.execute_input":"2024-10-27T09:04:12.658566Z","iopub.status.idle":"2024-10-27T09:04:14.627816Z","shell.execute_reply.started":"2024-10-27T09:04:12.658536Z","shell.execute_reply":"2024-10-27T09:04:14.627049Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# For using the new one\n# tokenizer = DebertaTokenizer.from_pretrained(\"microsoft/deberta-base\")\n\n# For loading the saved one\nmodel_dir = \"/kaggle/input/new-transformer-experiment-12-embedding-tmp/New_model\"\ntokenizer = DebertaTokenizer.from_pretrained(model_dir)","metadata":{"papermill":{"duration":0.127136,"end_time":"2024-10-20T09:45:03.907021","exception":false,"start_time":"2024-10-20T09:45:03.779885","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.628906Z","iopub.execute_input":"2024-10-27T09:04:14.629319Z","iopub.status.idle":"2024-10-27T09:04:14.788401Z","shell.execute_reply.started":"2024-10-27T09:04:14.629289Z","shell.execute_reply":"2024-10-27T09:04:14.787642Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 12\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 27\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"papermill":{"duration":0.0918,"end_time":"2024-10-20T09:45:04.016272","exception":false,"start_time":"2024-10-20T09:45:03.924472","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.789475Z","iopub.execute_input":"2024-10-27T09:04:14.789800Z","iopub.status.idle":"2024-10-27T09:04:14.825989Z","shell.execute_reply.started":"2024-10-27T09:04:14.789767Z","shell.execute_reply":"2024-10-27T09:04:14.824977Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"papermill":{"duration":0.031866,"end_time":"2024-10-20T09:45:04.066738","exception":false,"start_time":"2024-10-20T09:45:04.034872","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.827164Z","iopub.execute_input":"2024-10-27T09:04:14.827504Z","iopub.status.idle":"2024-10-27T09:04:14.836382Z","shell.execute_reply.started":"2024-10-27T09:04:14.827461Z","shell.execute_reply":"2024-10-27T09:04:14.835591Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"papermill":{"duration":0.032794,"end_time":"2024-10-20T09:45:04.120596","exception":false,"start_time":"2024-10-20T09:45:04.087802","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.837384Z","iopub.execute_input":"2024-10-27T09:04:14.837679Z","iopub.status.idle":"2024-10-27T09:04:14.848974Z","shell.execute_reply.started":"2024-10-27T09:04:14.837649Z","shell.execute_reply":"2024-10-27T09:04:14.848058Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\n\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\n\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"papermill":{"duration":107.917166,"end_time":"2024-10-20T09:46:52.053567","exception":false,"start_time":"2024-10-20T09:45:04.136401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.852284Z","iopub.execute_input":"2024-10-27T09:04:14.852651Z","iopub.status.idle":"2024-10-27T09:04:14.888674Z","shell.execute_reply.started":"2024-10-27T09:04:14.852614Z","shell.execute_reply":"2024-10-27T09:04:14.888000Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import DebertaModel, DebertaTokenizer\nfrom torch.distributions import Categorical\nimport os\n\nclass AdvancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n        super().__init__()\n        # For loading the new model\n        self.deberta = DebertaModel.from_pretrained(\"microsoft/deberta-base\")\n        self.hidden_size = hidden_size\n        # Classifiers for each hierarchy level\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # RL Policy networks for each level\n        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # Contrastive learning projection head\n        self.projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim)\n        )\n        # Few-shot learning prototypes\n        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n    def forward(self, input_ids, attention_mask):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n        # Supervised classification logits\n        supergroup_logits = self.supergroup_classifier(hidden_states)\n        group_input = torch.cat([hidden_states, torch.softmax(supergroup_logits, dim=1)], dim=1)\n        group_logits = self.group_classifier(group_input)\n        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n        module_logits = self.module_classifier(module_input)\n        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n        brand_logits = self.brand_classifier(brand_input)\n        # RL policy logits\n        supergroup_policy = self.supergroup_policy(hidden_states)\n        group_policy = self.group_policy(group_input)\n        module_policy = self.module_policy(module_input)\n        brand_policy = self.brand_policy(brand_input)\n        # Contrastive learning projection\n        projection = self.projection(hidden_states)\n        # Few-shot learning\n        prototype_distances = torch.cdist(hidden_states, self.prototypes)\n        few_shot_logits = -prototype_distances  # Negative distance as logits\n        return (supergroup_logits, group_logits, module_logits, brand_logits), \\\n               (supergroup_policy, group_policy, module_policy, brand_policy), \\\n               projection, few_shot_logits\n    def sample_actions(self, policies):\n        return [Categorical(logits=policy).sample() for policy in policies]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:04:14.889738Z","iopub.execute_input":"2024-10-27T09:04:14.890019Z","iopub.status.idle":"2024-10-27T09:04:14.905774Z","shell.execute_reply.started":"2024-10-27T09:04:14.889989Z","shell.execute_reply":"2024-10-27T09:04:14.903802Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import os\n\nclass JointAccuracyTrainer:\n    def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n        self.model = model\n        self.device = next(model.parameters()).device\n        self.supervised_optimizer = torch.optim.NAdam(model.parameters(), lr=supervised_lr)\n        self.rl_optimizer = torch.optim.NAdam(model.parameters(), lr=rl_lr)\n        self.criterion = nn.CrossEntropyLoss()\n        self.contrastive_temperature = contrastive_temperature\n        if loss_weights is None:\n            self.loss_weights = [1.0, 1.0, 1.0, 1.0]\n        else:\n            self.loss_weights = loss_weights\n\n    def compute_joint_loss(self, all_outputs, true_labels):\n        supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n        logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n        loss_supergroup = self.criterion(logits_supergroup, true_labels['supergroup'])\n        loss_group1 = self.criterion(logits_group1, true_labels['group1'])\n        loss_group2 = self.criterion(logits_group2, true_labels['group2'])\n        loss_group3 = self.criterion(logits_group3, true_labels['group3'])\n        total_loss = (\n            self.loss_weights[0] * loss_supergroup + \n            self.loss_weights[1] * loss_group1 + \n            self.loss_weights[2] * loss_group2 + \n            self.loss_weights[3] * loss_group3\n        )\n        return total_loss\n\n    def supervised_step(self, batch, true_labels):\n        self.supervised_optimizer.zero_grad()\n        all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n        total_loss = self.compute_joint_loss(all_outputs, true_labels)\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n        self.supervised_optimizer.step()\n        return total_loss.item()\n\n    def validation_step(self, batch, true_labels):\n        with torch.no_grad():\n            all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n            supervised_logits, _, _, _ = all_outputs\n            logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n            preds_supergroup = torch.argmax(logits_supergroup, dim=-1)\n            preds_group1 = torch.argmax(logits_group1, dim=-1)\n            preds_group2 = torch.argmax(logits_group2, dim=-1)\n            preds_group3 = torch.argmax(logits_group3, dim=-1)\n            supergroup_acc = (preds_supergroup == true_labels['supergroup']).float().mean().item()\n            group1_acc = (preds_group1 == true_labels['group1']).float().mean().item()\n            group2_acc = (preds_group2 == true_labels['group2']).float().mean().item()\n            group3_acc = (preds_group3 == true_labels['group3']).float().mean().item()\n            item_acc = ((preds_supergroup == true_labels['supergroup']) &\n                        (preds_group1 == true_labels['group1']) &\n                        (preds_group2 == true_labels['group2']) &\n                        (preds_group3 == true_labels['group3'])).float().mean().item()\n        return supergroup_acc, group1_acc, group2_acc, group3_acc, item_acc","metadata":{"papermill":{"duration":33669.587786,"end_time":"2024-10-20T19:08:01.658352","exception":false,"start_time":"2024-10-20T09:46:52.070566","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.906923Z","iopub.execute_input":"2024-10-27T09:04:14.907236Z","iopub.status.idle":"2024-10-27T09:04:14.923559Z","shell.execute_reply.started":"2024-10-27T09:04:14.907192Z","shell.execute_reply":"2024-10-27T09:04:14.922663Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Usage\nmodel = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\nmodel_path = os.path.join(model_dir, \"model.pth\")\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:04:59.880013Z","iopub.execute_input":"2024-10-27T09:04:59.880398Z","iopub.status.idle":"2024-10-27T09:05:11.458455Z","shell.execute_reply.started":"2024-10-27T09:04:59.880362Z","shell.execute_reply":"2024-10-27T09:05:11.457542Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca0c3d135ef4d068594f2330c5fec25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b03a5c6b3b40d69754411c1d4ea4f0"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_30/2259319365.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path))\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"# Profile on a single sample before starting the full training\nmodel.train()\nsample_batch = next(iter(train_loader))  # Get one sample batch\nsample_batch = {k: v.to(device) for k, v in sample_batch.items()}\ntrue_labels = {\n    'supergroup': sample_batch['labels1'],\n    'group1': sample_batch['labels2'],\n    'group2': sample_batch['labels3'],\n    'group3': sample_batch['labels4']\n}\ntrainer = JointAccuracyTrainer(model)\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n    _ = trainer.supervised_step(sample_batch, true_labels)\n\nprint(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\nprint(\"GFLOPs during training\") #GigaFLOPs\nprint(sum(k.flops for k in prof.key_averages())/1e9)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:05:52.441129Z","iopub.execute_input":"2024-10-27T09:05:52.441478Z","iopub.status.idle":"2024-10-27T09:05:59.015251Z","shell.execute_reply.started":"2024-10-27T09:05:52.441445Z","shell.execute_reply":"2024-10-27T09:05:59.014329Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n                                               aten::mm         2.60%      22.516ms         7.60%      65.957ms     372.636us      10.026ms         4.87%      10.026ms      56.642us           177     38758.970  \n                                            aten::addmm         2.05%      17.773ms         4.16%      36.053ms     621.610us       4.398ms         2.14%       4.398ms      75.834us            58     12871.145  \n                                              aten::bmm         0.87%       7.558ms         2.27%      19.732ms     137.026us       1.926ms         0.94%       1.926ms      13.378us           144       382.206  \n                                              aten::mul         1.41%      12.201ms         3.48%      30.219ms      57.124us     919.741us         0.45%     919.741us       1.739us           529        32.456  \n                                              aten::add         0.71%       6.149ms         0.91%       7.909ms      41.845us     575.323us         0.28%     575.323us       3.044us           189         7.731  \n                    Optimizer.zero_grad#NAdam.zero_grad         0.02%     153.977us         0.02%     153.977us     153.977us       0.000us         0.00%       0.000us       0.000us             1            --  \n                                            aten::zeros         0.30%       2.621ms         2.48%      21.490ms     397.963us       0.000us         0.00%     388.831us       7.201us            54            --  \n                                            aten::empty         0.62%       5.372ms         0.76%       6.636ms       6.716us       0.000us         0.00%       0.000us       0.000us           988            --  \n                                            aten::zero_         0.21%       1.798ms         3.00%      26.046ms      51.780us       0.000us         0.00%       3.104ms       6.170us           503            --  \n                                            aten::fill_         0.56%       4.877ms         2.80%      24.299ms      48.021us       3.110ms         1.51%       3.110ms       6.145us           506            --  \n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \nSelf CPU time total: 867.567ms\nSelf CUDA time total: 205.878ms\n\nGFLOPs during training\n52.052508185\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=10):\n    trainer = JointAccuracyTrainer(model)\n    best_item_accuracy = 0.0  # Track best item accuracy\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_sup_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            true_labels = {\n                'supergroup': batch['labels1'],\n                'group1': batch['labels2'],\n                'group2': batch['labels3'],\n                'group3': batch['labels4']\n            }\n            sup_loss = trainer.supervised_step(batch, true_labels)\n            total_sup_loss += sup_loss\n\n        model.eval()\n        val_accuracies = [0, 0, 0, 0, 0]\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            true_labels = {\n                'supergroup': batch['labels1'],\n                'group1': batch['labels2'],\n                'group2': batch['labels3'],\n                'group3': batch['labels4']\n            }\n            accs = trainer.validation_step(batch, true_labels)\n            val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n\n        val_accuracies = [x / len(val_loader) for x in val_accuracies]\n        item_accuracy = val_accuracies[4]  # Current epoch's item accuracy\n\n        # Check for best item accuracy and save model if improved\n        if item_accuracy > best_item_accuracy:\n            best_item_accuracy = item_accuracy\n            model_save_path = os.path.join(\"best_model.pth\")\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"New best model saved with item accuracy: {best_item_accuracy:.4f}\")\n            \n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n              f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, Group1: {val_accuracies[1]:.4f}, \"\n              f\"Group2: {val_accuracies[2]:.4f}, Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}\")\n\ntrain_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:06:02.783277Z","iopub.execute_input":"2024-10-27T09:06:02.784403Z","iopub.status.idle":"2024-10-27T09:06:03.167294Z","shell.execute_reply.started":"2024-10-27T09:06:02.784361Z","shell.execute_reply":"2024-10-27T09:06:03.166214Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Epoch 1/2 - Train Loss: 75.2980, Val Accuracies - Supergroup: 1.0000, Group1: 0.0000, Group2: 0.0000, Group3: 0.0000, Item Accuracy: 0.0000\nEpoch 2/2 - Train Loss: 68.2193, Val Accuracies - Supergroup: 1.0000, Group1: 0.0000, Group2: 0.0000, Group3: 0.0000, Item Accuracy: 0.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n# Create the directory if it doesn't exist\n\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n# Save the model's state dictionary\n\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n# Save the tokenizer\n\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"papermill":{"duration":1.131605,"end_time":"2024-10-20T19:08:02.807942","exception":false,"start_time":"2024-10-20T19:08:01.676337","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:06:03.169672Z","iopub.execute_input":"2024-10-27T09:06:03.170132Z","iopub.status.idle":"2024-10-27T09:06:04.235388Z","shell.execute_reply.started":"2024-10-27T09:06:03.170084Z","shell.execute_reply":"2024-10-27T09:06:04.234444Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model saved to New_model/model.pth\nTokenizer saved to New_model\n","output_type":"stream"}]},{"cell_type":"code","source":"try:\n    model.load_state_dict(torch.load(os.path.join(\"best_model.pth\")))\nexcept:\n    print(\"Nothing to worry about\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:06:04.236540Z","iopub.execute_input":"2024-10-27T09:06:04.236877Z","iopub.status.idle":"2024-10-27T09:06:04.778112Z","shell.execute_reply.started":"2024-10-27T09:06:04.236843Z","shell.execute_reply":"2024-10-27T09:06:04.776630Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/998431601.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(os.path.join(\"best_model.pth\")))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model.pth'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'best_model.pth'","output_type":"error"}]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"papermill":{"duration":0.872266,"end_time":"2024-10-20T19:08:03.698211","exception":false,"start_time":"2024-10-20T19:08:02.825945","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:06:50.771657Z","iopub.execute_input":"2024-10-27T09:06:50.772080Z","iopub.status.idle":"2024-10-27T09:06:51.569071Z","shell.execute_reply.started":"2024-10-27T09:06:50.772045Z","shell.execute_reply":"2024-10-27T09:06:51.568180Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_test_feat.head()","metadata":{"papermill":{"duration":0.037505,"end_time":"2024-10-20T19:08:03.753727","exception":false,"start_time":"2024-10-20T19:08:03.716222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:06:51.570967Z","iopub.execute_input":"2024-10-27T09:06:51.571342Z","iopub.status.idle":"2024-10-27T09:06:51.586080Z","shell.execute_reply.started":"2024-10-27T09:06:51.571301Z","shell.execute_reply":"2024-10-27T09:06:51.584961Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   indoml_id                    description retailer  price\n0          0             14 in hybrid blade    wilko   4.50\n1          1         2 pk vent stick a fres  noshify   0.69\n2          2               4 tyrefix 450 ml  noshify   2.99\n3          3           4 x 4 tyrefix 450 ml  noshify   2.99\n4          4  5 l adbluescr diesel vehicles  noshify   4.99","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>indoml_id</th>\n      <th>description</th>\n      <th>retailer</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>14 in hybrid blade</td>\n      <td>wilko</td>\n      <td>4.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2 pk vent stick a fres</td>\n      <td>noshify</td>\n      <td>0.69</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>4 tyrefix 450 ml</td>\n      <td>noshify</td>\n      <td>2.99</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4 x 4 tyrefix 450 ml</td>\n      <td>noshify</td>\n      <td>2.99</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5 l adbluescr diesel vehicles</td>\n      <td>noshify</td>\n      <td>4.99</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def predict(model, tokenizer, text):\n    start = time.time()\n    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n        inputs = tokenizer(\n            text, \n            return_tensors=\"pt\", \n            truncation=True, \n            padding='max_length', \n            max_length=MAX_LENGTH\n        ).to(DEVICE)\n        with torch.no_grad():\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n        predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n        \n    print(\"Inference time :\"+str(time.time()-start))\n    #print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n    print(\"GFLOPs during testing\") #GigaFLOPs\n    print(sum(k.flops for k in prof.key_averages())/1e9)\n    return predictions\n\n# Example prediction\nsample_text = \"14 in hybrid blade\"\npredictions = predict(model, tokenizer, sample_text)\nprint(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")","metadata":{"papermill":{"duration":0.069928,"end_time":"2024-10-20T19:08:03.842254","exception":false,"start_time":"2024-10-20T19:08:03.772326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:09:54.434987Z","iopub.execute_input":"2024-10-27T09:09:54.435835Z","iopub.status.idle":"2024-10-27T09:09:54.977447Z","shell.execute_reply.started":"2024-10-27T09:09:54.435787Z","shell.execute_reply":"2024-10-27T09:09:54.976556Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Inference time :0.3503093719482422\nGFLOPs during testing\n2.782020956\nSupergroup: 23, Group: 180, Module: 327, Brand: 1987\n","output_type":"stream"}]},{"cell_type":"code","source":"# df_test_feat = df_test_feat[:5]","metadata":{"papermill":{"duration":0.025659,"end_time":"2024-10-20T19:08:03.886458","exception":false,"start_time":"2024-10-20T19:08:03.860799","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:09:58.902224Z","iopub.execute_input":"2024-10-27T09:09:58.902582Z","iopub.status.idle":"2024-10-27T09:09:58.907468Z","shell.execute_reply.started":"2024-10-27T09:09:58.902542Z","shell.execute_reply":"2024-10-27T09:09:58.906428Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(0, len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    with torch.no_grad():\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n            inputs = tokenizer(\n                df_test_feat.iloc[i].description, \n                return_tensors=\"pt\", \n                truncation=True, \n                padding='max_length', \n                max_length=MAX_LENGTH\n            ).to(DEVICE)\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n                \n        try:\n            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n        except ValueError as e:\n            print(f\"Error in supergroups: {e}\")\n            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n        try:\n            groups_names = group_encoder.inverse_transform(groups_list)\n        except ValueError as e:\n            print(f\"Error in groups: {e}\")\n            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n        try:\n            modules_names = module_encoder.inverse_transform(modules_list)\n        except ValueError as e:\n            print(f\"Error in modules: {e}\")\n            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n        try:\n            brands_names = brand_encoder.inverse_transform(brands_list)\n        except ValueError as e:\n            print(f\"Error in brands: {e}\")\n            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n        # Create a DataFrame with predictions\n        predictions_df = pd.DataFrame({\n            'indoml_id': indoml_id_list,\n            'supergroup': supergroups_names,\n            'group': groups_names,\n            'module': modules_names,\n            'brand': brands_names\n        })\n        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n        print(\"predictions.predict saved\")\nprint(make_test_pred_and_save(df_test_feat))","metadata":{"papermill":{"duration":4252.569523,"end_time":"2024-10-20T20:18:56.474256","exception":false,"start_time":"2024-10-20T19:08:03.904733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:09:59.060480Z","iopub.execute_input":"2024-10-27T09:09:59.060836Z","iopub.status.idle":"2024-10-27T09:09:59.214540Z","shell.execute_reply.started":"2024-10-27T09:09:59.060796Z","shell.execute_reply":"2024-10-27T09:09:59.213548Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Processing 0 of 4\nError in supergroups: y contains previously unseen labels: [ 5  6 19 23]\nError in groups: y contains previously unseen labels: [  2  44  74 152 180]\nError in modules: y contains previously unseen labels: [ 18  89 327 384]\nError in brands: y contains previously unseen labels: [1987 4078]\npredictions.predict saved\nNone\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.031783,"end_time":"2024-10-20T20:18:56.538258","exception":false,"start_time":"2024-10-20T20:18:56.506475","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}