{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9642895,"sourceType":"datasetVersion","datasetId":5779523},{"sourceId":9733083,"sourceType":"datasetVersion","datasetId":5920554}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":29566.386963,"end_time":"2024-10-22T16:15:48.124790","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-22T08:03:01.737827","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"13d006268b9543719b9b469875232456":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c389e1588be4bdc8a96a2e98fe9a39c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"312bda717fe545e8a50812276ddd4114":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3851f9ae12c94904a13a934ff43566fb","placeholder":"​","style":"IPY_MODEL_db6df2ab18174fe789bc0269a6daf550","value":" 1.12G/1.12G [00:03&lt;00:00, 358MB/s]"}},"34a9c217ac0f4794a9d6392b6821b6d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3851f9ae12c94904a13a934ff43566fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b4c3b11dfc84639846234e75b3d476f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"730a84fc1c5d47249726753e4d0c8d52":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8382d2c11ba547fda9193ccf018791e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a599409ceef4e84be4ae3cde84f119f","IPY_MODEL_a49ac36562c14c1c990d563abe81fa53","IPY_MODEL_312bda717fe545e8a50812276ddd4114"],"layout":"IPY_MODEL_c361c9b1a4734166a83a16049df07620"}},"879ccc7154d642219702f58851953f63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a599409ceef4e84be4ae3cde84f119f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_730a84fc1c5d47249726753e4d0c8d52","placeholder":"​","style":"IPY_MODEL_d681f6216b8c4854a7dccd36e6ab2d26","value":"model.safetensors: 100%"}},"97f697eb1aad40be99697cf284aa82e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6ad9abebc1740528e6deb66c70918b5","IPY_MODEL_a5d545e569654fe8b5961955ceca2cba","IPY_MODEL_bce5630e1a36449cb1c5110d7e600000"],"layout":"IPY_MODEL_5b4c3b11dfc84639846234e75b3d476f"}},"a49ac36562c14c1c990d563abe81fa53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_879ccc7154d642219702f58851953f63","max":1115567652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c389e1588be4bdc8a96a2e98fe9a39c","value":1115567652}},"a5d545e569654fe8b5961955ceca2cba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ace53662dd294453b90ecbbbbf50d8a8","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ed6cc2645bcd4c0c9637393716935b2a","value":615}},"ace53662dd294453b90ecbbbbf50d8a8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bce5630e1a36449cb1c5110d7e600000":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa02fbc4813b44e7afb700fcaf658e80","placeholder":"​","style":"IPY_MODEL_34a9c217ac0f4794a9d6392b6821b6d3","value":" 615/615 [00:00&lt;00:00, 49.1kB/s]"}},"c361c9b1a4734166a83a16049df07620":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d681f6216b8c4854a7dccd36e6ab2d26":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db6df2ab18174fe789bc0269a6daf550":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e4937e9e50004dadbef8785fa407536d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed6cc2645bcd4c0c9637393716935b2a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6ad9abebc1740528e6deb66c70918b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13d006268b9543719b9b469875232456","placeholder":"​","style":"IPY_MODEL_e4937e9e50004dadbef8785fa407536d","value":"config.json: 100%"}},"fa02fbc4813b44e7afb700fcaf658e80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Do not use the weights from v4 of the notebook, i.e. v3 version of weights, trained on retailer thing</h1>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.metrics import accuracy_score\ndf_features = pd.read_json('/kaggle/input/indoml-phase2/train.features',lines=True)\ndf_labels = pd.read_json('/kaggle/input/indoml-phase2/train.labels',lines=True)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":11.243174,"end_time":"2024-10-22T08:03:15.687636","exception":false,"start_time":"2024-10-22T08:03:04.444462","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:01.337561Z","iopub.execute_input":"2024-10-27T09:04:01.337968Z","iopub.status.idle":"2024-10-27T09:04:11.521269Z","shell.execute_reply.started":"2024-10-27T09:04:01.337927Z","shell.execute_reply":"2024-10-27T09:04:11.520273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nhuggingface_token = \"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\"\nlogin(token=huggingface_token)","metadata":{"papermill":{"duration":0.648049,"end_time":"2024-10-22T08:03:16.353663","exception":false,"start_time":"2024-10-22T08:03:15.705614","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:11.522876Z","iopub.execute_input":"2024-10-27T09:04:11.523182Z","iopub.status.idle":"2024-10-27T09:04:12.064741Z","shell.execute_reply.started":"2024-10-27T09:04:11.523149Z","shell.execute_reply":"2024-10-27T09:04:12.063834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(df_features,df_labels,on=\"indoml_id\")\n# df = df[:10]","metadata":{"papermill":{"duration":0.169884,"end_time":"2024-10-22T08:03:16.540009","exception":false,"start_time":"2024-10-22T08:03:16.370125","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.066061Z","iopub.execute_input":"2024-10-27T09:04:12.066753Z","iopub.status.idle":"2024-10-27T09:04:12.216149Z","shell.execute_reply.started":"2024-10-27T09:04:12.066707Z","shell.execute_reply":"2024-10-27T09:04:12.215351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ngroup_encoder = LabelEncoder()\nsupergroup_encoder = LabelEncoder()\nmodule_encoder = LabelEncoder()\nbrand_encoder = LabelEncoder()\n# Fit and transform each column\ndf['group'] = group_encoder.fit_transform(df['group'])\ndf['supergroup'] = supergroup_encoder.fit_transform(df['supergroup'])\ndf['module'] = module_encoder.fit_transform(df['module'])\ndf['brand'] = brand_encoder.fit_transform(df['brand'])","metadata":{"papermill":{"duration":0.91745,"end_time":"2024-10-22T08:03:17.513137","exception":false,"start_time":"2024-10-22T08:03:16.595687","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.218852Z","iopub.execute_input":"2024-10-27T09:04:12.219503Z","iopub.status.idle":"2024-10-27T09:04:12.257027Z","shell.execute_reply.started":"2024-10-27T09:04:12.219455Z","shell.execute_reply":"2024-10-27T09:04:12.256020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, XLMRobertaModel\n# from transformers import XLNetTokenizer, XLNetModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom torch.distributions import Categorical\nimport numpy as np\nimport time\nfrom torch.profiler import profile, record_function, ProfilerActivity","metadata":{"papermill":{"duration":1.758718,"end_time":"2024-10-22T08:03:19.288527","exception":false,"start_time":"2024-10-22T08:03:17.529809","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:12.258503Z","iopub.execute_input":"2024-10-27T09:04:12.259131Z","iopub.status.idle":"2024-10-27T09:04:13.780600Z","shell.execute_reply.started":"2024-10-27T09:04:12.259086Z","shell.execute_reply":"2024-10-27T09:04:13.779798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For using the new one\n# tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n# For loading the saved one\nmodel_dir = \"/kaggle/input/new-transformer-experiment-12-emb-xlm-roberta-tmp/New_model\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)","metadata":{"papermill":{"duration":0.888874,"end_time":"2024-10-22T08:03:20.194432","exception":false,"start_time":"2024-10-22T08:03:19.305558","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:13.781724Z","iopub.execute_input":"2024-10-27T09:04:13.782162Z","iopub.status.idle":"2024-10-27T09:04:14.605017Z","shell.execute_reply.started":"2024-10-27T09:04:13.782128Z","shell.execute_reply":"2024-10-27T09:04:14.604235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH = 12\nBATCH_SIZE = 64\nLEARNING_RATE = 5e-5\nNUM_EPOCHS = 27\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice=DEVICE\nPATIENCE = 5  # Early stopping patience\nPATIENCE_LR = 3  # Reduce LR on plateau patience","metadata":{"papermill":{"duration":0.094411,"end_time":"2024-10-22T08:03:20.310804","exception":false,"start_time":"2024-10-22T08:03:20.216393","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.606112Z","iopub.execute_input":"2024-10-27T09:04:14.606439Z","iopub.status.idle":"2024-10-27T09:04:14.640071Z","shell.execute_reply.started":"2024-10-27T09:04:14.606405Z","shell.execute_reply":"2024-10-27T09:04:14.639101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport torch","metadata":{"papermill":{"duration":0.023144,"end_time":"2024-10-22T08:03:20.350287","exception":false,"start_time":"2024-10-22T08:03:20.327143","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.641206Z","iopub.execute_input":"2024-10-27T09:04:14.641500Z","iopub.status.idle":"2024-10-27T09:04:14.650507Z","shell.execute_reply.started":"2024-10-27T09:04:14.641469Z","shell.execute_reply":"2024-10-27T09:04:14.649680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ProductDataset(Dataset):\n    def __init__(self, texts, labels1, labels2, labels3, labels4):\n        self.texts = texts\n        self.labels1 = labels1\n        self.labels2 = labels2\n        self.labels3 = labels3\n        self.labels4 = labels4\n        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\")\n    def __len__(self):\n        return len(self.texts)\n    def __getitem__(self, idx):\n        item = {key: val[idx].to(DEVICE) for key, val in self.encodings.items()}\n        item['labels1'] = torch.tensor(self.labels1[idx], device=DEVICE)\n        item['labels2'] = torch.tensor(self.labels2[idx], device=DEVICE)\n        item['labels3'] = torch.tensor(self.labels3[idx], device=DEVICE)\n        item['labels4'] = torch.tensor(self.labels4[idx], device=DEVICE)\n        return item\n        \ndef compute_accuracy(preds, labels):\n    # Convert each tensor in the list to numpy arrays\n    preds_np = [p.cpu().numpy() for p in preds]\n    labels_np = [l.cpu().numpy() for l in labels]\n    # Individual accuracies for each of the 4 labels\n    accuracies = [accuracy_score(labels_np[i], preds_np[i]) for i in range(4)]\n    # Overall accuracy where all 4 labels match\n    overall_accuracy = accuracy_score(\n        np.all([labels_np[i] == preds_np[i] for i in range(4)], axis=0), \n        np.ones(len(labels_np[0]))\n    )\n    # Return the 5 accuracies (4 individual, 1 overall)\n    return accuracies + [overall_accuracy]","metadata":{"papermill":{"duration":0.030793,"end_time":"2024-10-22T08:03:20.397520","exception":false,"start_time":"2024-10-22T08:03:20.366727","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.651671Z","iopub.execute_input":"2024-10-27T09:04:14.651956Z","iopub.status.idle":"2024-10-27T09:04:14.664194Z","shell.execute_reply.started":"2024-10-27T09:04:14.651909Z","shell.execute_reply":"2024-10-27T09:04:14.663384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data\ntrain_texts, val_texts, train_labels1, val_labels1, train_labels2, val_labels2, train_labels3, val_labels3, train_labels4, val_labels4 = train_test_split(\n    df['description'], \n    df['supergroup'], \n    df['group'], \n    df['module'], \n    df['brand'], \n    test_size=0.2, \n    random_state=42\n)\ntrain_dataset = ProductDataset(\n    train_texts.tolist(), \n    train_labels1.tolist(), \n    train_labels2.tolist(), \n    train_labels3.tolist(), \n    train_labels4.tolist()\n)\nval_dataset = ProductDataset(\n    val_texts.tolist(), \n    val_labels1.tolist(), \n    val_labels2.tolist(), \n    val_labels3.tolist(), \n    val_labels4.tolist()\n)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"papermill":{"duration":21.816104,"end_time":"2024-10-22T08:03:42.229766","exception":false,"start_time":"2024-10-22T08:03:20.413662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.667199Z","iopub.execute_input":"2024-10-27T09:04:14.667562Z","iopub.status.idle":"2024-10-27T09:04:14.698117Z","shell.execute_reply.started":"2024-10-27T09:04:14.667531Z","shell.execute_reply":"2024-10-27T09:04:14.697434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from transformers import DebertaModel, DebertaTokenizer\nfrom transformers import XLNetTokenizer, XLMRobertaModel\nfrom torch.distributions import Categorical\nimport os\nclass AdvancedHierarchicalClassifier(nn.Module):\n    def __init__(self, num_supergroups, num_groups, num_modules, num_brands, hidden_size=768, projection_dim=128):\n        super().__init__()\n        # For loading the new model\n        # self.model = XLNetModel.from_pretrained('xlnet-base-cased')\n        self.model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n        self.hidden_size = hidden_size\n        # Classifiers for each hierarchy level\n        self.supergroup_classifier = nn.Linear(hidden_size, num_supergroups)\n        self.group_classifier = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_classifier = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_classifier = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # RL Policy networks for each level\n        self.supergroup_policy = nn.Linear(hidden_size, num_supergroups)\n        self.group_policy = nn.Linear(hidden_size + num_supergroups, num_groups)\n        self.module_policy = nn.Linear(hidden_size + num_supergroups + num_groups, num_modules)\n        self.brand_policy = nn.Linear(hidden_size + num_supergroups + num_groups + num_modules, num_brands)\n        # Contrastive learning projection head\n        self.projection = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, projection_dim)\n        )\n        # Few-shot learning prototypes\n        self.prototypes = nn.Parameter(torch.randn(num_supergroups + num_groups + num_modules + num_brands, hidden_size))\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token representation\n        # Supervised classification logits\n        supergroup_logits = self.supergroup_classifier(hidden_states)\n        group_input = torch.cat([hidden_states, torch.softmax(supergroup_logits, dim=1)], dim=1)\n        group_logits = self.group_classifier(group_input)\n        module_input = torch.cat([group_input, torch.softmax(group_logits, dim=1)], dim=1)\n        module_logits = self.module_classifier(module_input)\n        brand_input = torch.cat([module_input, torch.softmax(module_logits, dim=1)], dim=1)\n        brand_logits = self.brand_classifier(brand_input)\n        # RL policy logits\n        supergroup_policy = self.supergroup_policy(hidden_states)\n        group_policy = self.group_policy(group_input)\n        module_policy = self.module_policy(module_input)\n        brand_policy = self.brand_policy(brand_input)\n        # Contrastive learning projection\n        projection = self.projection(hidden_states)\n        # Few-shot learning\n        prototype_distances = torch.cdist(hidden_states, self.prototypes)\n        few_shot_logits = -prototype_distances  # Negative distance as logits\n        return (supergroup_logits, group_logits, module_logits, brand_logits), \\\n               (supergroup_policy, group_policy, module_policy, brand_policy), \\\n               projection, few_shot_logits\n    def sample_actions(self, policies):\n        return [Categorical(logits=policy).sample() for policy in policies]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:04:14.699311Z","iopub.execute_input":"2024-10-27T09:04:14.699586Z","iopub.status.idle":"2024-10-27T09:04:14.716779Z","shell.execute_reply.started":"2024-10-27T09:04:14.699556Z","shell.execute_reply":"2024-10-27T09:04:14.715891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nclass JointAccuracyTrainer:\n    def __init__(self, model, supervised_lr=1e-5, rl_lr=1e-4, contrastive_temperature=0.07, loss_weights=None):\n        self.model = model\n        self.device = next(model.parameters()).device\n        self.supervised_optimizer = torch.optim.NAdam(model.parameters(), lr=supervised_lr)\n        self.rl_optimizer = torch.optim.NAdam(model.parameters(), lr=rl_lr)\n        self.criterion = nn.CrossEntropyLoss()\n        self.contrastive_temperature = contrastive_temperature\n        if loss_weights is None:\n            self.loss_weights = [1.0, 1.0, 1.0, 1.0]\n        else:\n            self.loss_weights = loss_weights\n\n    def compute_joint_loss(self, all_outputs, true_labels):\n        supervised_logits, policy_logits, projection, few_shot_logits = all_outputs\n        logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n        loss_supergroup = self.criterion(logits_supergroup, true_labels['supergroup'])\n        loss_group1 = self.criterion(logits_group1, true_labels['group1'])\n        loss_group2 = self.criterion(logits_group2, true_labels['group2'])\n        loss_group3 = self.criterion(logits_group3, true_labels['group3'])\n        total_loss = (\n            self.loss_weights[0] * loss_supergroup + \n            self.loss_weights[1] * loss_group1 + \n            self.loss_weights[2] * loss_group2 + \n            self.loss_weights[3] * loss_group3\n        )\n        return total_loss\n\n    def supervised_step(self, batch, true_labels):\n        self.supervised_optimizer.zero_grad()\n        all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n        total_loss = self.compute_joint_loss(all_outputs, true_labels)\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n        self.supervised_optimizer.step()\n        return total_loss.item()\n\n    def validation_step(self, batch, true_labels):\n        with torch.no_grad():\n            all_outputs = self.model(batch['input_ids'], batch['attention_mask'])\n            supervised_logits, _, _, _ = all_outputs\n            logits_supergroup, logits_group1, logits_group2, logits_group3 = supervised_logits\n            preds_supergroup = torch.argmax(logits_supergroup, dim=-1)\n            preds_group1 = torch.argmax(logits_group1, dim=-1)\n            preds_group2 = torch.argmax(logits_group2, dim=-1)\n            preds_group3 = torch.argmax(logits_group3, dim=-1)\n            supergroup_acc = (preds_supergroup == true_labels['supergroup']).float().mean().item()\n            group1_acc = (preds_group1 == true_labels['group1']).float().mean().item()\n            group2_acc = (preds_group2 == true_labels['group2']).float().mean().item()\n            group3_acc = (preds_group3 == true_labels['group3']).float().mean().item()\n            item_acc = ((preds_supergroup == true_labels['supergroup']) &\n                        (preds_group1 == true_labels['group1']) &\n                        (preds_group2 == true_labels['group2']) &\n                        (preds_group3 == true_labels['group3'])).float().mean().item()\n        return supergroup_acc, group1_acc, group2_acc, group3_acc, item_acc","metadata":{"papermill":{"duration":27747.264877,"end_time":"2024-10-22T15:46:09.511938","exception":false,"start_time":"2024-10-22T08:03:42.247061","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:04:14.717981Z","iopub.execute_input":"2024-10-27T09:04:14.718276Z","iopub.status.idle":"2024-10-27T09:04:14.734417Z","shell.execute_reply.started":"2024-10-27T09:04:14.718236Z","shell.execute_reply":"2024-10-27T09:04:14.733593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usage\nmodel = AdvancedHierarchicalClassifier(num_supergroups=32, num_groups=228, num_modules=449, num_brands=5679).to(DEVICE)\nmodel_path = os.path.join(model_dir, \"model.pth\")\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:04:43.124759Z","iopub.execute_input":"2024-10-27T09:04:43.125635Z","iopub.status.idle":"2024-10-27T09:04:57.683441Z","shell.execute_reply.started":"2024-10-27T09:04:43.125595Z","shell.execute_reply":"2024-10-27T09:04:57.682507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Profile on a single sample before starting the full training\nmodel.train()\nsample_batch = next(iter(train_loader))  # Get one sample batch\nsample_batch = {k: v.to(device) for k, v in sample_batch.items()}\ntrue_labels = {\n    'supergroup': sample_batch['labels1'],\n    'group1': sample_batch['labels2'],\n    'group2': sample_batch['labels3'],\n    'group3': sample_batch['labels4']\n}\ntrainer = JointAccuracyTrainer(model)\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], with_flops=True) as prof:\n    _ = trainer.supervised_step(sample_batch, true_labels)\n\nprint(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\nprint(\"GFLOPs during training\") #GigaFLOPs\nprint(sum(k.flops for k in prof.key_averages())/1e9)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:06:17.249235Z","iopub.execute_input":"2024-10-27T09:06:17.249654Z","iopub.status.idle":"2024-10-27T09:06:22.077158Z","shell.execute_reply.started":"2024-10-27T09:06:17.249618Z","shell.execute_reply":"2024-10-27T09:06:22.076162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=10):\n    trainer = JointAccuracyTrainer(model)\n    best_item_accuracy = 0.0  # Track best item accuracy\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_sup_loss = 0.0\n        for batch in train_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            true_labels = {\n                'supergroup': batch['labels1'],\n                'group1': batch['labels2'],\n                'group2': batch['labels3'],\n                'group3': batch['labels4']\n            }\n            sup_loss = trainer.supervised_step(batch, true_labels)\n            total_sup_loss += sup_loss\n\n        model.eval()\n        val_accuracies = [0, 0, 0, 0, 0]\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            true_labels = {\n                'supergroup': batch['labels1'],\n                'group1': batch['labels2'],\n                'group2': batch['labels3'],\n                'group3': batch['labels4']\n            }\n            accs = trainer.validation_step(batch, true_labels)\n            val_accuracies = [sum(x) for x in zip(val_accuracies, accs)]\n\n        val_accuracies = [x / len(val_loader) for x in val_accuracies]\n        item_accuracy = val_accuracies[4]  # Current epoch's item accuracy\n        \n        # Check for best item accuracy and save model if improved\n        if item_accuracy > best_item_accuracy:\n            best_item_accuracy = item_accuracy\n            model_save_path = os.path.join(\"best_model.pth\")\n            torch.save(model.state_dict(), model_save_path)\n            print(f\"New best model saved with item accuracy: {best_item_accuracy:.4f}\")\n\n        print(f\"Epoch {epoch + 1}/{num_epochs} - \"\n              f\"Train Loss: {total_sup_loss / len(train_loader):.4f}, \"\n              f\"Val Accuracies - Supergroup: {val_accuracies[0]:.4f}, Group1: {val_accuracies[1]:.4f}, \"\n              f\"Group2: {val_accuracies[2]:.4f}, Group3: {val_accuracies[3]:.4f}, Item Accuracy: {val_accuracies[4]:.4f}\")\n\ntrain_and_evaluate(model, train_loader, val_loader, model_dir, num_epochs=NUM_EPOCHS)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:07:40.065862Z","iopub.execute_input":"2024-10-27T09:07:40.066808Z","iopub.status.idle":"2024-10-27T09:07:40.310285Z","shell.execute_reply.started":"2024-10-27T09:07:40.066763Z","shell.execute_reply":"2024-10-27T09:07:40.309356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# Define the directory to save the model and tokenizer\nsave_directory = \"New_model\"\n# Create the directory if it doesn't exist\nif not os.path.exists(save_directory):\n    os.makedirs(save_directory)\n# Save the model's state dictionary\nmodel_save_path = os.path.join(save_directory, \"model.pth\")\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n# Save the tokenizer\ntokenizer.save_pretrained(save_directory)\nprint(f\"Tokenizer saved to {save_directory}\")","metadata":{"papermill":{"duration":2.277296,"end_time":"2024-10-22T15:46:11.808971","exception":false,"start_time":"2024-10-22T15:46:09.531675","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:07:15.198589Z","iopub.execute_input":"2024-10-27T09:07:15.199232Z","iopub.status.idle":"2024-10-27T09:07:17.419333Z","shell.execute_reply.started":"2024-10-27T09:07:15.199185Z","shell.execute_reply":"2024-10-27T09:07:17.418261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    model.load_state_dict(torch.load(os.path.join(\"best_model.pth\")))\nexcept:\n    print(\"Nothing to worry about\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T09:07:17.420705Z","iopub.execute_input":"2024-10-27T09:07:17.421087Z","iopub.status.idle":"2024-10-27T09:07:17.701050Z","shell.execute_reply.started":"2024-10-27T09:07:17.421044Z","shell.execute_reply":"2024-10-27T09:07:17.699786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat = pd.read_json('/kaggle/input/indoml-phase2/final_test_data.features',lines=True)","metadata":{"papermill":{"duration":1.154524,"end_time":"2024-10-22T15:46:12.982154","exception":false,"start_time":"2024-10-22T15:46:11.827630","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:07:44.762034Z","iopub.execute_input":"2024-10-27T09:07:44.762918Z","iopub.status.idle":"2024-10-27T09:07:45.526694Z","shell.execute_reply.started":"2024-10-27T09:07:44.762874Z","shell.execute_reply":"2024-10-27T09:07:45.525604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_feat.head()","metadata":{"papermill":{"duration":0.092465,"end_time":"2024-10-22T15:46:13.094142","exception":false,"start_time":"2024-10-22T15:46:13.001677","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:07:45.528562Z","iopub.execute_input":"2024-10-27T09:07:45.528888Z","iopub.status.idle":"2024-10-27T09:07:45.545187Z","shell.execute_reply.started":"2024-10-27T09:07:45.528854Z","shell.execute_reply":"2024-10-27T09:07:45.544164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, tokenizer, text):\n    start = time.time()\n    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],with_flops=True) as prof:\n        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n        predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n        \n    print(\"Inference time :\"+str(time.time()-start))\n    #print(prof.key_averages().table(sort_by=\"flops\",row_limit=10))\n    print(\"GFLOPs during testing\") #GigaFLOPs\n    print(sum(k.flops for k in prof.key_averages())/1e9)\n        \n    return predictions\n# Example prediction\nsample_text = \"14 in hybrid blade\"\npredictions = predict(model, tokenizer, sample_text)\nprint(f\"Supergroup: {predictions[0]}, Group: {predictions[1]}, Module: {predictions[2]}, Brand: {predictions[3]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df_test_feat = df_test_feat[:5]","metadata":{"papermill":{"duration":0.025788,"end_time":"2024-10-22T15:46:13.233023","exception":false,"start_time":"2024-10-22T15:46:13.207235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:09:16.710426Z","iopub.execute_input":"2024-10-27T09:09:16.712629Z","iopub.status.idle":"2024-10-27T09:09:16.720806Z","shell.execute_reply.started":"2024-10-27T09:09:16.712569Z","shell.execute_reply":"2024-10-27T09:09:16.719725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_test_pred_and_save(df_test_feat):\n    supergroups_list = []\n    groups_list = []\n    modules_list = []\n    brands_list = []\n    indoml_id_list = range(0, len(df_test_feat))\n    length_df = df_test_feat.shape[0]\n    with torch.no_grad():\n        start = time.time()\n        for i in range(length_df):\n            if i % 1000 == 0:\n                print(f\"Processing {i} of {length_df - 1}\")\n            inputs = tokenizer(df_test_feat.iloc[i].description, truncation=True, padding='max_length', max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n            logits, _, _, _ = model(inputs['input_ids'], inputs['attention_mask'])\n            predictions = [torch.argmax(logit, dim=1).item() for logit in logits]\n            # Append predictions to respective lists\n            supergroups_list.append(predictions[0])\n            groups_list.append(predictions[1])\n            modules_list.append(predictions[2])\n            brands_list.append(predictions[3])\n        \n        try:\n            supergroups_names = supergroup_encoder.inverse_transform(supergroups_list)\n        except ValueError as e:\n            print(f\"Error in supergroups: {e}\")\n            supergroups_names = ['Unknown' if x not in supergroup_encoder.classes_ else x for x in supergroups_list]\n        try:\n            groups_names = group_encoder.inverse_transform(groups_list)\n        except ValueError as e:\n            print(f\"Error in groups: {e}\")\n            groups_names = ['Unknown' if x not in group_encoder.classes_ else x for x in groups_list]\n        try:\n            modules_names = module_encoder.inverse_transform(modules_list)\n        except ValueError as e:\n            print(f\"Error in modules: {e}\")\n            modules_names = ['Unknown' if x not in module_encoder.classes_ else x for x in modules_list]\n        try:\n            brands_names = brand_encoder.inverse_transform(brands_list)\n        except ValueError as e:\n            print(f\"Error in brands: {e}\")\n            brands_names = ['Unknown' if x not in brand_encoder.classes_ else x for x in brands_list]\n        # Create a DataFrame with predictions\n        predictions_df = pd.DataFrame({\n            'indoml_id': indoml_id_list,\n            'supergroup': supergroups_names,\n            'group': groups_names,\n            'module': modules_names,\n            'brand': brands_names\n        })\n        predictions_df.to_json('/kaggle/working/predictions.predict', orient='records', lines=True)\n        print(\"predictions.predict saved\")\nprint(make_test_pred_and_save(df_test_feat))","metadata":{"papermill":{"duration":1771.8885,"end_time":"2024-10-22T16:15:45.141076","exception":false,"start_time":"2024-10-22T15:46:13.252576","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-27T09:09:35.813441Z","iopub.execute_input":"2024-10-27T09:09:35.813843Z","iopub.status.idle":"2024-10-27T09:09:35.891734Z","shell.execute_reply.started":"2024-10-27T09:09:35.813804Z","shell.execute_reply":"2024-10-27T09:09:35.890844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.032737,"end_time":"2024-10-22T16:15:45.207415","exception":false,"start_time":"2024-10-22T16:15:45.174678","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.032684,"end_time":"2024-10-22T16:15:45.273106","exception":false,"start_time":"2024-10-22T16:15:45.240422","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}